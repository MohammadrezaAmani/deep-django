{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Actual App instance implementation.\"\"\"\n",
    "import inspect\n",
    "import os\n",
    "import sys\n",
    "import threading\n",
    "import warnings\n",
    "from collections import UserDict, defaultdict, deque\n",
    "from datetime import datetime\n",
    "from datetime import timezone as datetime_timezone\n",
    "from operator import attrgetter\n",
    "\n",
    "from click.exceptions import Exit\n",
    "from dateutil.parser import isoparse\n",
    "from kombu import pools\n",
    "from kombu.clocks import LamportClock\n",
    "from kombu.common import oid_from\n",
    "from kombu.utils.compat import register_after_fork\n",
    "from kombu.utils.objects import cached_property\n",
    "from kombu.utils.uuid import uuid\n",
    "from vine import starpromise\n",
    "\n",
    "from celery import platforms, signals\n",
    "from celery._state import (\n",
    "    _announce_app_finalized,\n",
    "    _deregister_app,\n",
    "    _register_app,\n",
    "    _set_current_app,\n",
    "    _task_stack,\n",
    "    connect_on_app_finalize,\n",
    "    get_current_app,\n",
    "    get_current_worker_task,\n",
    "    set_default_app,\n",
    ")\n",
    "from celery.exceptions import AlwaysEagerIgnored, ImproperlyConfigured\n",
    "from celery.loaders import get_loader_cls\n",
    "from celery.local import PromiseProxy, maybe_evaluate\n",
    "from celery.utils import abstract\n",
    "from celery.utils.collections import AttributeDictMixin\n",
    "from celery.utils.dispatch import Signal\n",
    "from celery.utils.functional import first, head_from_fun, maybe_list\n",
    "from celery.utils.imports import gen_task_name, instantiate, symbol_by_name\n",
    "from celery.utils.log import get_logger\n",
    "from celery.utils.objects import FallbackContext, mro_lookup\n",
    "from celery.utils.time import maybe_make_aware, timezone, to_utc\n",
    "\n",
    "# Load all builtin tasks\n",
    "from . import backends, builtins  # noqa\n",
    "from .annotations import prepare as prepare_annotations\n",
    "from .autoretry import add_autoretry_behaviour\n",
    "from .defaults import DEFAULT_SECURITY_DIGEST, find_deprecated_settings\n",
    "from .registry import TaskRegistry\n",
    "from .utils import (\n",
    "    AppPickler,\n",
    "    Settings,\n",
    "    _new_key_to_old,\n",
    "    _old_key_to_new,\n",
    "    _unpickle_app,\n",
    "    _unpickle_app_v2,\n",
    "    appstr,\n",
    "    bugreport,\n",
    "    detect_settings,\n",
    ")\n",
    "\n",
    "__all__ = (\"Celery\",)\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "BUILTIN_FIXUPS = {\n",
    "    \"celery.fixups.django:fixup\",\n",
    "}\n",
    "USING_EXECV = os.environ.get(\"FORKED_BY_MULTIPROCESSING\")\n",
    "\n",
    "ERR_ENVVAR_NOT_SET = \"\"\"\n",
    "The environment variable {0!r} is not set,\n",
    "and as such the configuration could not be loaded.\n",
    "\n",
    "Please set this variable and make sure it points to\n",
    "a valid configuration module.\n",
    "\n",
    "Example:\n",
    "    {0}=\"proj.celeryconfig\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def app_has_custom(app, attr):\n",
    "    \"\"\"Return true if app has customized method `attr`.\n",
    "\n",
    "    Note:\n",
    "        This is used for optimizations in cases where we know\n",
    "        how the default behavior works, but need to account\n",
    "        for someone using inheritance to override a method/property.\n",
    "    \"\"\"\n",
    "    return mro_lookup(\n",
    "        app.__class__, attr, stop={Celery, object}, monkey_patched=[__name__]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _unpickle_appattr(reverse_name, args):\n",
    "    \"\"\"Unpickle app.\"\"\"\n",
    "    # Given an attribute name and a list of args, gets\n",
    "    # the attribute from the current app and calls it.\n",
    "    return get_current_app()._rgetattr(reverse_name)(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _after_fork_cleanup_app(app):\n",
    "    # This is used with multiprocessing.register_after_fork,\n",
    "    # so need to be at module level.\n",
    "    try:\n",
    "        app._after_fork()\n",
    "    except Exception as exc:  # pylint: disable=broad-except\n",
    "        logger.info(\"after forker raised exception: %r\", exc, exc_info=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PendingConfiguration(UserDict, AttributeDictMixin):\n",
    "    # `app.conf` will be of this type before being explicitly configured,\n",
    "    # meaning the app can keep any configuration set directly\n",
    "    # on `app.conf` before the `app.config_from_object` call.\n",
    "    #\n",
    "    # accessing any key will finalize the configuration,\n",
    "    # replacing `app.conf` with a concrete settings object.\n",
    "\n",
    "    callback = None\n",
    "    _data = None\n",
    "\n",
    "    def __init__(self, conf, callback):\n",
    "        object.__setattr__(self, \"_data\", conf)\n",
    "        object.__setattr__(self, \"callback\", callback)\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        self._data[key] = value\n",
    "\n",
    "    def clear(self):\n",
    "        self._data.clear()\n",
    "\n",
    "    def update(self, *args, **kwargs):\n",
    "        self._data.update(*args, **kwargs)\n",
    "\n",
    "    def setdefault(self, *args, **kwargs):\n",
    "        return self._data.setdefault(*args, **kwargs)\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        # XXX will not show finalized configuration\n",
    "        # setdefault will cause `key in d` to happen,\n",
    "        # so for setdefault to be lazy, so does contains.\n",
    "        return key in self._data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self.data)\n",
    "\n",
    "    @cached_property\n",
    "    def data(self):\n",
    "        return self.callback()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Celery:\n",
    "    \"\"\"Celery application.\n",
    "\n",
    "    Arguments:\n",
    "        main (str): Name of the main module if running as `__main__`.\n",
    "            This is used as the prefix for auto-generated task names.\n",
    "\n",
    "    Keyword Arguments:\n",
    "        broker (str): URL of the default broker used.\n",
    "        backend (Union[str, Type[celery.backends.base.Backend]]):\n",
    "            The result store backend class, or the name of the backend\n",
    "            class to use.\n",
    "\n",
    "            Default is the value of the :setting:`result_backend` setting.\n",
    "        autofinalize (bool): If set to False a :exc:`RuntimeError`\n",
    "            will be raised if the task registry or tasks are used before\n",
    "            the app is finalized.\n",
    "        set_as_current (bool):  Make this the global current app.\n",
    "        include (List[str]): List of modules every worker should import.\n",
    "\n",
    "        amqp (Union[str, Type[AMQP]]): AMQP object or class name.\n",
    "        events (Union[str, Type[celery.app.events.Events]]): Events object or\n",
    "            class name.\n",
    "        log (Union[str, Type[Logging]]): Log object or class name.\n",
    "        control (Union[str, Type[celery.app.control.Control]]): Control object\n",
    "            or class name.\n",
    "        tasks (Union[str, Type[TaskRegistry]]): A task registry, or the name of\n",
    "            a registry class.\n",
    "        fixups (List[str]): List of fix-up plug-ins (e.g., see\n",
    "            :mod:`celery.fixups.django`).\n",
    "        config_source (Union[str, class]): Take configuration from a class,\n",
    "            or object.  Attributes may include any settings described in\n",
    "            the documentation.\n",
    "        task_cls (Union[str, Type[celery.app.task.Task]]): base task class to\n",
    "            use. See :ref:`this section <custom-task-cls-app-wide>` for usage.\n",
    "    \"\"\"\n",
    "\n",
    "    #: This is deprecated, use :meth:`reduce_keys` instead\n",
    "    Pickler = AppPickler\n",
    "\n",
    "    SYSTEM = platforms.SYSTEM\n",
    "    IS_macOS, IS_WINDOWS = platforms.IS_macOS, platforms.IS_WINDOWS\n",
    "\n",
    "    #: Name of the `__main__` module.  Required for standalone scripts.\n",
    "    #:\n",
    "    #: If set this will be used instead of `__main__` when automatically\n",
    "    #: generating task names.\n",
    "    main = None\n",
    "\n",
    "    #: Custom options for command-line programs.\n",
    "    #: See :ref:`extending-commandoptions`\n",
    "    user_options = None\n",
    "\n",
    "    #: Custom bootsteps to extend and modify the worker.\n",
    "    #: See :ref:`extending-bootsteps`.\n",
    "    steps = None\n",
    "\n",
    "    builtin_fixups = BUILTIN_FIXUPS\n",
    "\n",
    "    amqp_cls = \"celery.app.amqp:AMQP\"\n",
    "    backend_cls = None\n",
    "    events_cls = \"celery.app.events:Events\"\n",
    "    loader_cls = None\n",
    "    log_cls = \"celery.app.log:Logging\"\n",
    "    control_cls = \"celery.app.control:Control\"\n",
    "    task_cls = \"celery.app.task:Task\"\n",
    "    registry_cls = \"celery.app.registry:TaskRegistry\"\n",
    "\n",
    "    #: Thread local storage.\n",
    "    _local = None\n",
    "    _fixups = None\n",
    "    _pool = None\n",
    "    _conf = None\n",
    "    _after_fork_registered = False\n",
    "\n",
    "    #: Signal sent when app is loading configuration.\n",
    "    on_configure = None\n",
    "\n",
    "    #: Signal sent after app has prepared the configuration.\n",
    "    on_after_configure = None\n",
    "\n",
    "    #: Signal sent after app has been finalized.\n",
    "    on_after_finalize = None\n",
    "\n",
    "    #: Signal sent by every new process after fork.\n",
    "    on_after_fork = None\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        main=None,\n",
    "        loader=None,\n",
    "        backend=None,\n",
    "        amqp=None,\n",
    "        events=None,\n",
    "        log=None,\n",
    "        control=None,\n",
    "        set_as_current=True,\n",
    "        tasks=None,\n",
    "        broker=None,\n",
    "        include=None,\n",
    "        changes=None,\n",
    "        config_source=None,\n",
    "        fixups=None,\n",
    "        task_cls=None,\n",
    "        autofinalize=True,\n",
    "        namespace=None,\n",
    "        strict_typing=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self._local = threading.local()\n",
    "        self._backend_cache = None\n",
    "\n",
    "        self.clock = LamportClock()\n",
    "        self.main = main\n",
    "        self.amqp_cls = amqp or self.amqp_cls\n",
    "        self.events_cls = events or self.events_cls\n",
    "        self.loader_cls = loader or self._get_default_loader()\n",
    "        self.log_cls = log or self.log_cls\n",
    "        self.control_cls = control or self.control_cls\n",
    "        self._custom_task_cls_used = bool(task_cls)\n",
    "        self.task_cls = task_cls or self.task_cls\n",
    "        self.set_as_current = set_as_current\n",
    "        self.registry_cls = symbol_by_name(self.registry_cls)\n",
    "        self.user_options = defaultdict(set)\n",
    "        self.steps = defaultdict(set)\n",
    "        self.autofinalize = autofinalize\n",
    "        self.namespace = namespace\n",
    "        self.strict_typing = strict_typing\n",
    "\n",
    "        self.configured = False\n",
    "        self._config_source = config_source\n",
    "        self._pending_defaults = deque()\n",
    "        self._pending_periodic_tasks = deque()\n",
    "\n",
    "        self.finalized = False\n",
    "        self._finalize_mutex = threading.RLock()\n",
    "        self._pending = deque()\n",
    "        self._tasks = tasks\n",
    "        if not isinstance(self._tasks, TaskRegistry):\n",
    "            self._tasks = self.registry_cls(self._tasks or {})\n",
    "\n",
    "        # If the class defines a custom __reduce_args__ we need to use\n",
    "        # the old way of pickling apps: pickling a list of\n",
    "        # args instead of the new way that pickles a dict of keywords.\n",
    "        self._using_v1_reduce = app_has_custom(self, \"__reduce_args__\")\n",
    "\n",
    "        # these options are moved to the config to\n",
    "        # simplify pickling of the app object.\n",
    "        self._preconf = changes or {}\n",
    "        self._preconf_set_by_auto = set()\n",
    "        self.__autoset(\"broker_url\", broker)\n",
    "        self.__autoset(\"result_backend\", backend)\n",
    "        self.__autoset(\"include\", include)\n",
    "\n",
    "        for key, value in kwargs.items():\n",
    "            self.__autoset(key, value)\n",
    "\n",
    "        self._conf = Settings(\n",
    "            PendingConfiguration(self._preconf, self._finalize_pending_conf),\n",
    "            prefix=self.namespace,\n",
    "            keys=(_old_key_to_new, _new_key_to_old),\n",
    "        )\n",
    "\n",
    "        # - Apply fix-ups.\n",
    "        self.fixups = set(self.builtin_fixups) if fixups is None else fixups\n",
    "        # ...store fixup instances in _fixups to keep weakrefs alive.\n",
    "        self._fixups = [symbol_by_name(fixup)(self) for fixup in self.fixups]\n",
    "\n",
    "        if self.set_as_current:\n",
    "            self.set_current()\n",
    "\n",
    "        # Signals\n",
    "        if self.on_configure is None:\n",
    "            # used to be a method pre 4.0\n",
    "            self.on_configure = Signal(name=\"app.on_configure\")\n",
    "        self.on_after_configure = Signal(\n",
    "            name=\"app.on_after_configure\",\n",
    "            providing_args={\"source\"},\n",
    "        )\n",
    "        self.on_after_finalize = Signal(name=\"app.on_after_finalize\")\n",
    "        self.on_after_fork = Signal(name=\"app.on_after_fork\")\n",
    "\n",
    "        # Boolean signalling, whether fast_trace_task are enabled.\n",
    "        # this attribute is set in celery.worker.trace and checked by celery.worker.request\n",
    "        self.use_fast_trace_task = False\n",
    "\n",
    "        self.on_init()\n",
    "        _register_app(self)\n",
    "\n",
    "    def _get_default_loader(self):\n",
    "        # the --loader command-line argument sets the environment variable.\n",
    "        return (\n",
    "            os.environ.get(\"CELERY_LOADER\")\n",
    "            or self.loader_cls\n",
    "            or \"celery.loaders.app:AppLoader\"\n",
    "        )\n",
    "\n",
    "    def on_init(self):\n",
    "        \"\"\"Optional callback called at init.\"\"\"\n",
    "\n",
    "    def __autoset(self, key, value):\n",
    "        if value is not None:\n",
    "            self._preconf[key] = value\n",
    "            self._preconf_set_by_auto.add(key)\n",
    "\n",
    "    def set_current(self):\n",
    "        \"\"\"Make this the current app for this thread.\"\"\"\n",
    "        _set_current_app(self)\n",
    "\n",
    "    def set_default(self):\n",
    "        \"\"\"Make this the default app for all threads.\"\"\"\n",
    "        set_default_app(self)\n",
    "\n",
    "    def _ensure_after_fork(self):\n",
    "        if not self._after_fork_registered:\n",
    "            self._after_fork_registered = True\n",
    "            if register_after_fork is not None:\n",
    "                register_after_fork(self, _after_fork_cleanup_app)\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Clean up after the application.\n",
    "\n",
    "        Only necessary for dynamically created apps, and you should\n",
    "        probably use the :keyword:`with` statement instead.\n",
    "\n",
    "        Example:\n",
    "            >>> with Celery(set_as_current=False) as app:\n",
    "            ...     with app.connection_for_write() as conn:\n",
    "            ...         pass\n",
    "        \"\"\"\n",
    "        self._pool = None\n",
    "        _deregister_app(self)\n",
    "\n",
    "    def start(self, argv=None):\n",
    "        \"\"\"Run :program:`celery` using `argv`.\n",
    "\n",
    "        Uses :data:`sys.argv` if `argv` is not specified.\n",
    "        \"\"\"\n",
    "        from celery.bin.celery import celery\n",
    "\n",
    "        celery.params[0].default = self\n",
    "\n",
    "        if argv is None:\n",
    "            argv = sys.argv\n",
    "\n",
    "        try:\n",
    "            celery.main(args=argv, standalone_mode=False)\n",
    "        except Exit as e:\n",
    "            return e.exit_code\n",
    "        finally:\n",
    "            celery.params[0].default = None\n",
    "\n",
    "    def worker_main(self, argv=None):\n",
    "        \"\"\"Run :program:`celery worker` using `argv`.\n",
    "\n",
    "        Uses :data:`sys.argv` if `argv` is not specified.\n",
    "        \"\"\"\n",
    "        if argv is None:\n",
    "            argv = sys.argv\n",
    "\n",
    "        if \"worker\" not in argv:\n",
    "            raise ValueError(\n",
    "                \"The worker sub-command must be specified in argv.\\n\"\n",
    "                \"Use app.start() to programmatically start other commands.\"\n",
    "            )\n",
    "\n",
    "        self.start(argv=argv)\n",
    "\n",
    "    def task(self, *args, **opts):\n",
    "        \"\"\"Decorator to create a task class out of any callable.\n",
    "\n",
    "        See :ref:`Task options<task-options>` for a list of the\n",
    "        arguments that can be passed to this decorator.\n",
    "\n",
    "        Examples:\n",
    "            .. code-block:: python\n",
    "\n",
    "                @app.task\n",
    "                def refresh_feed(url):\n",
    "                    store_feed(feedparser.parse(url))\n",
    "\n",
    "            with setting extra options:\n",
    "\n",
    "            .. code-block:: python\n",
    "\n",
    "                @app.task(exchange='feeds')\n",
    "                def refresh_feed(url):\n",
    "                    return store_feed(feedparser.parse(url))\n",
    "\n",
    "        Note:\n",
    "            App Binding: For custom apps the task decorator will return\n",
    "            a proxy object, so that the act of creating the task is not\n",
    "            performed until the task is used or the task registry is accessed.\n",
    "\n",
    "            If you're depending on binding to be deferred, then you must\n",
    "            not access any attributes on the returned object until the\n",
    "            application is fully set up (finalized).\n",
    "        \"\"\"\n",
    "        if USING_EXECV and opts.get(\"lazy\", True):\n",
    "            # When using execv the task in the original module will point to a\n",
    "            # different app, so doing things like 'add.request' will point to\n",
    "            # a different task instance.  This makes sure it will always use\n",
    "            # the task instance from the current app.\n",
    "            # Really need a better solution for this :(\n",
    "            from . import shared_task\n",
    "\n",
    "            return shared_task(*args, lazy=False, **opts)\n",
    "\n",
    "        def inner_create_task_cls(shared=True, filter=None, lazy=True, **opts):\n",
    "            _filt = filter\n",
    "\n",
    "            def _create_task_cls(fun):\n",
    "                if shared:\n",
    "\n",
    "                    def cons(app):\n",
    "                        return app._task_from_fun(fun, **opts)\n",
    "\n",
    "                    cons.__name__ = fun.__name__\n",
    "                    connect_on_app_finalize(cons)\n",
    "                if not lazy or self.finalized:\n",
    "                    ret = self._task_from_fun(fun, **opts)\n",
    "                else:\n",
    "                    # return a proxy object that evaluates on first use\n",
    "                    ret = PromiseProxy(\n",
    "                        self._task_from_fun, (fun,), opts, __doc__=fun.__doc__\n",
    "                    )\n",
    "                    self._pending.append(ret)\n",
    "                if _filt:\n",
    "                    return _filt(ret)\n",
    "                return ret\n",
    "\n",
    "            return _create_task_cls\n",
    "\n",
    "        if len(args) == 1:\n",
    "            if callable(args[0]):\n",
    "                return inner_create_task_cls(**opts)(*args)\n",
    "            raise TypeError(\"argument 1 to @task() must be a callable\")\n",
    "        if args:\n",
    "            raise TypeError(\n",
    "                \"@task() takes exactly 1 argument ({} given)\".format(\n",
    "                    sum([len(args), len(opts)])\n",
    "                )\n",
    "            )\n",
    "        return inner_create_task_cls(**opts)\n",
    "\n",
    "    def type_checker(self, fun, bound=False):\n",
    "        return staticmethod(head_from_fun(fun, bound=bound))\n",
    "\n",
    "    def _task_from_fun(self, fun, name=None, base=None, bind=False, **options):\n",
    "        if not self.finalized and not self.autofinalize:\n",
    "            raise RuntimeError(\"Contract breach: app not finalized\")\n",
    "        name = name or self.gen_task_name(fun.__name__, fun.__module__)\n",
    "        base = base or self.Task\n",
    "\n",
    "        if name not in self._tasks:\n",
    "            run = fun if bind else staticmethod(fun)\n",
    "            task = type(\n",
    "                fun.__name__,\n",
    "                (base,),\n",
    "                dict(\n",
    "                    {\n",
    "                        \"app\": self,\n",
    "                        \"name\": name,\n",
    "                        \"run\": run,\n",
    "                        \"_decorated\": True,\n",
    "                        \"__doc__\": fun.__doc__,\n",
    "                        \"__module__\": fun.__module__,\n",
    "                        \"__annotations__\": fun.__annotations__,\n",
    "                        \"__header__\": self.type_checker(fun, bound=bind),\n",
    "                        \"__wrapped__\": run,\n",
    "                    },\n",
    "                    **options,\n",
    "                ),\n",
    "            )()\n",
    "            # for some reason __qualname__ cannot be set in type()\n",
    "            # so we have to set it here.\n",
    "            try:\n",
    "                task.__qualname__ = fun.__qualname__\n",
    "            except AttributeError:\n",
    "                pass\n",
    "            self._tasks[task.name] = task\n",
    "            task.bind(self)  # connects task to this app\n",
    "            add_autoretry_behaviour(task, **options)\n",
    "        else:\n",
    "            task = self._tasks[name]\n",
    "        return task\n",
    "\n",
    "    def register_task(self, task, **options):\n",
    "        \"\"\"Utility for registering a task-based class.\n",
    "\n",
    "        Note:\n",
    "            This is here for compatibility with old Celery 1.0\n",
    "            style task classes, you should not need to use this for\n",
    "            new projects.\n",
    "        \"\"\"\n",
    "        task = inspect.isclass(task) and task() or task\n",
    "        if not task.name:\n",
    "            task_cls = type(task)\n",
    "            task.name = self.gen_task_name(task_cls.__name__, task_cls.__module__)\n",
    "        add_autoretry_behaviour(task, **options)\n",
    "        self.tasks[task.name] = task\n",
    "        task._app = self\n",
    "        task.bind(self)\n",
    "        return task\n",
    "\n",
    "    def gen_task_name(self, name, module):\n",
    "        return gen_task_name(self, name, module)\n",
    "\n",
    "    def finalize(self, auto=False):\n",
    "        \"\"\"Finalize the app.\n",
    "\n",
    "        This loads built-in tasks, evaluates pending task decorators,\n",
    "        reads configuration, etc.\n",
    "        \"\"\"\n",
    "        with self._finalize_mutex:\n",
    "            if not self.finalized:\n",
    "                if auto and not self.autofinalize:\n",
    "                    raise RuntimeError(\"Contract breach: app not finalized\")\n",
    "                self.finalized = True\n",
    "                _announce_app_finalized(self)\n",
    "\n",
    "                pending = self._pending\n",
    "                while pending:\n",
    "                    maybe_evaluate(pending.popleft())\n",
    "\n",
    "                for task in self._tasks.values():\n",
    "                    task.bind(self)\n",
    "\n",
    "                self.on_after_finalize.send(sender=self)\n",
    "\n",
    "    def add_defaults(self, fun):\n",
    "        \"\"\"Add default configuration from dict ``d``.\n",
    "\n",
    "        If the argument is a callable function then it will be regarded\n",
    "        as a promise, and it won't be loaded until the configuration is\n",
    "        actually needed.\n",
    "\n",
    "        This method can be compared to:\n",
    "\n",
    "        .. code-block:: pycon\n",
    "\n",
    "            >>> celery.conf.update(d)\n",
    "\n",
    "        with a difference that 1) no copy will be made and 2) the dict will\n",
    "        not be transferred when the worker spawns child processes, so\n",
    "        it's important that the same configuration happens at import time\n",
    "        when pickle restores the object on the other side.\n",
    "        \"\"\"\n",
    "        if not callable(fun):\n",
    "            d, fun = fun, lambda: d\n",
    "        if self.configured:\n",
    "            return self._conf.add_defaults(fun())\n",
    "        self._pending_defaults.append(fun)\n",
    "\n",
    "    def config_from_object(self, obj, silent=False, force=False, namespace=None):\n",
    "        \"\"\"Read configuration from object.\n",
    "\n",
    "        Object is either an actual object or the name of a module to import.\n",
    "\n",
    "        Example:\n",
    "            >>> celery.config_from_object('myapp.celeryconfig')\n",
    "\n",
    "            >>> from myapp import celeryconfig\n",
    "            >>> celery.config_from_object(celeryconfig)\n",
    "\n",
    "        Arguments:\n",
    "            silent (bool): If true then import errors will be ignored.\n",
    "            force (bool): Force reading configuration immediately.\n",
    "                By default the configuration will be read only when required.\n",
    "        \"\"\"\n",
    "        self._config_source = obj\n",
    "        self.namespace = namespace or self.namespace\n",
    "        if force or self.configured:\n",
    "            self._conf = None\n",
    "            if self.loader.config_from_object(obj, silent=silent):\n",
    "                return self.conf\n",
    "\n",
    "    def config_from_envvar(self, variable_name, silent=False, force=False):\n",
    "        \"\"\"Read configuration from environment variable.\n",
    "\n",
    "        The value of the environment variable must be the name\n",
    "        of a module to import.\n",
    "\n",
    "        Example:\n",
    "            >>> os.environ['CELERY_CONFIG_MODULE'] = 'myapp.celeryconfig'\n",
    "            >>> celery.config_from_envvar('CELERY_CONFIG_MODULE')\n",
    "        \"\"\"\n",
    "        module_name = os.environ.get(variable_name)\n",
    "        if not module_name:\n",
    "            if silent:\n",
    "                return False\n",
    "            raise ImproperlyConfigured(ERR_ENVVAR_NOT_SET.strip().format(variable_name))\n",
    "        return self.config_from_object(module_name, silent=silent, force=force)\n",
    "\n",
    "    def config_from_cmdline(self, argv, namespace=\"celery\"):\n",
    "        self._conf.update(self.loader.cmdline_config_parser(argv, namespace))\n",
    "\n",
    "    def setup_security(\n",
    "        self,\n",
    "        allowed_serializers=None,\n",
    "        key=None,\n",
    "        key_password=None,\n",
    "        cert=None,\n",
    "        store=None,\n",
    "        digest=DEFAULT_SECURITY_DIGEST,\n",
    "        serializer=\"json\",\n",
    "    ):\n",
    "        \"\"\"Setup the message-signing serializer.\n",
    "\n",
    "        This will affect all application instances (a global operation).\n",
    "\n",
    "        Disables untrusted serializers and if configured to use the ``auth``\n",
    "        serializer will register the ``auth`` serializer with the provided\n",
    "        settings into the Kombu serializer registry.\n",
    "\n",
    "        Arguments:\n",
    "            allowed_serializers (Set[str]): List of serializer names, or\n",
    "                content_types that should be exempt from being disabled.\n",
    "            key (str): Name of private key file to use.\n",
    "                Defaults to the :setting:`security_key` setting.\n",
    "            key_password (bytes): Password to decrypt the private key.\n",
    "                Defaults to the :setting:`security_key_password` setting.\n",
    "            cert (str): Name of certificate file to use.\n",
    "                Defaults to the :setting:`security_certificate` setting.\n",
    "            store (str): Directory containing certificates.\n",
    "                Defaults to the :setting:`security_cert_store` setting.\n",
    "            digest (str): Digest algorithm used when signing messages.\n",
    "                Default is ``sha256``.\n",
    "            serializer (str): Serializer used to encode messages after\n",
    "                they've been signed.  See :setting:`task_serializer` for\n",
    "                the serializers supported.  Default is ``json``.\n",
    "        \"\"\"\n",
    "        from celery.security import setup_security\n",
    "\n",
    "        return setup_security(\n",
    "            allowed_serializers,\n",
    "            key,\n",
    "            key_password,\n",
    "            cert,\n",
    "            store,\n",
    "            digest,\n",
    "            serializer,\n",
    "            app=self,\n",
    "        )\n",
    "\n",
    "    def autodiscover_tasks(self, packages=None, related_name=\"tasks\", force=False):\n",
    "        \"\"\"Auto-discover task modules.\n",
    "\n",
    "        Searches a list of packages for a \"tasks.py\" module (or use\n",
    "        related_name argument).\n",
    "\n",
    "        If the name is empty, this will be delegated to fix-ups (e.g., Django).\n",
    "\n",
    "        For example if you have a directory layout like this:\n",
    "\n",
    "        .. code-block:: text\n",
    "\n",
    "            foo/__init__.py\n",
    "               tasks.py\n",
    "               models.py\n",
    "\n",
    "            bar/__init__.py\n",
    "                tasks.py\n",
    "                models.py\n",
    "\n",
    "            baz/__init__.py\n",
    "                models.py\n",
    "\n",
    "        Then calling ``app.autodiscover_tasks(['foo', 'bar', 'baz'])`` will\n",
    "        result in the modules ``foo.tasks`` and ``bar.tasks`` being imported.\n",
    "\n",
    "        Arguments:\n",
    "            packages (List[str]): List of packages to search.\n",
    "                This argument may also be a callable, in which case the\n",
    "                value returned is used (for lazy evaluation).\n",
    "            related_name (Optional[str]): The name of the module to find.  Defaults\n",
    "                to \"tasks\": meaning \"look for 'module.tasks' for every\n",
    "                module in ``packages``.\".  If ``None`` will only try to import\n",
    "                the package, i.e. \"look for 'module'\".\n",
    "            force (bool): By default this call is lazy so that the actual\n",
    "                auto-discovery won't happen until an application imports\n",
    "                the default modules.  Forcing will cause the auto-discovery\n",
    "                to happen immediately.\n",
    "        \"\"\"\n",
    "        if force:\n",
    "            return self._autodiscover_tasks(packages, related_name)\n",
    "        signals.import_modules.connect(\n",
    "            starpromise(\n",
    "                self._autodiscover_tasks,\n",
    "                packages,\n",
    "                related_name,\n",
    "            ),\n",
    "            weak=False,\n",
    "            sender=self,\n",
    "        )\n",
    "\n",
    "    def _autodiscover_tasks(self, packages, related_name, **kwargs):\n",
    "        if packages:\n",
    "            return self._autodiscover_tasks_from_names(packages, related_name)\n",
    "        return self._autodiscover_tasks_from_fixups(related_name)\n",
    "\n",
    "    def _autodiscover_tasks_from_names(self, packages, related_name):\n",
    "        # packages argument can be lazy\n",
    "        return self.loader.autodiscover_tasks(\n",
    "            packages() if callable(packages) else packages,\n",
    "            related_name,\n",
    "        )\n",
    "\n",
    "    def _autodiscover_tasks_from_fixups(self, related_name):\n",
    "        return self._autodiscover_tasks_from_names(\n",
    "            [\n",
    "                pkg\n",
    "                for fixup in self._fixups\n",
    "                if hasattr(fixup, \"autodiscover_tasks\")\n",
    "                for pkg in fixup.autodiscover_tasks()\n",
    "            ],\n",
    "            related_name=related_name,\n",
    "        )\n",
    "\n",
    "    def send_task(\n",
    "        self,\n",
    "        name,\n",
    "        args=None,\n",
    "        kwargs=None,\n",
    "        countdown=None,\n",
    "        eta=None,\n",
    "        task_id=None,\n",
    "        producer=None,\n",
    "        connection=None,\n",
    "        router=None,\n",
    "        result_cls=None,\n",
    "        expires=None,\n",
    "        publisher=None,\n",
    "        link=None,\n",
    "        link_error=None,\n",
    "        add_to_parent=True,\n",
    "        group_id=None,\n",
    "        group_index=None,\n",
    "        retries=0,\n",
    "        chord=None,\n",
    "        reply_to=None,\n",
    "        time_limit=None,\n",
    "        soft_time_limit=None,\n",
    "        root_id=None,\n",
    "        parent_id=None,\n",
    "        route_name=None,\n",
    "        shadow=None,\n",
    "        chain=None,\n",
    "        task_type=None,\n",
    "        replaced_task_nesting=0,\n",
    "        **options,\n",
    "    ):\n",
    "        \"\"\"Send task by name.\n",
    "\n",
    "        Supports the same arguments as :meth:`@-Task.apply_async`.\n",
    "\n",
    "        Arguments:\n",
    "            name (str): Name of task to call (e.g., `\"tasks.add\"`).\n",
    "            result_cls (AsyncResult): Specify custom result class.\n",
    "        \"\"\"\n",
    "        parent = have_parent = None\n",
    "        amqp = self.amqp\n",
    "        task_id = task_id or uuid()\n",
    "        producer = producer or publisher  # XXX compat\n",
    "        router = router or amqp.router\n",
    "        conf = self.conf\n",
    "        if conf.task_always_eager:  # pragma: no cover\n",
    "            warnings.warn(\n",
    "                AlwaysEagerIgnored(\n",
    "                    \"task_always_eager has no effect on send_task\",\n",
    "                ),\n",
    "                stacklevel=2,\n",
    "            )\n",
    "\n",
    "        ignore_result = options.pop(\"ignore_result\", False)\n",
    "        options = router.route(options, route_name or name, args, kwargs, task_type)\n",
    "        if expires is not None:\n",
    "            if isinstance(expires, datetime):\n",
    "                expires_s = (maybe_make_aware(expires) - self.now()).total_seconds()\n",
    "            elif isinstance(expires, str):\n",
    "                expires_s = (\n",
    "                    maybe_make_aware(isoparse(expires)) - self.now()\n",
    "                ).total_seconds()\n",
    "            else:\n",
    "                expires_s = expires\n",
    "\n",
    "            if expires_s < 0:\n",
    "                logger.warning(\n",
    "                    f\"{task_id} has an expiration date in the past ({-expires_s}s ago).\\n\"\n",
    "                    \"We assume this is intended and so we have set the \"\n",
    "                    \"expiration date to 0 instead.\\n\"\n",
    "                    \"According to RabbitMQ's documentation:\\n\"\n",
    "                    '\"Setting the TTL to 0 causes messages to be expired upon '\n",
    "                    \"reaching a queue unless they can be delivered to a \"\n",
    "                    'consumer immediately.\"\\n'\n",
    "                    \"If this was unintended, please check the code which \"\n",
    "                    \"published this task.\"\n",
    "                )\n",
    "                expires_s = 0\n",
    "\n",
    "            options[\"expiration\"] = expires_s\n",
    "\n",
    "        if not root_id or not parent_id:\n",
    "            parent = self.current_worker_task\n",
    "            if parent:\n",
    "                if not root_id:\n",
    "                    root_id = parent.request.root_id or parent.request.id\n",
    "                if not parent_id:\n",
    "                    parent_id = parent.request.id\n",
    "\n",
    "                if conf.task_inherit_parent_priority:\n",
    "                    options.setdefault(\n",
    "                        \"priority\", parent.request.delivery_info.get(\"priority\")\n",
    "                    )\n",
    "\n",
    "        # alias for 'task_as_v2'\n",
    "        message = amqp.create_task_message(\n",
    "            task_id,\n",
    "            name,\n",
    "            args,\n",
    "            kwargs,\n",
    "            countdown,\n",
    "            eta,\n",
    "            group_id,\n",
    "            group_index,\n",
    "            expires,\n",
    "            retries,\n",
    "            chord,\n",
    "            maybe_list(link),\n",
    "            maybe_list(link_error),\n",
    "            reply_to or self.thread_oid,\n",
    "            time_limit,\n",
    "            soft_time_limit,\n",
    "            self.conf.task_send_sent_event,\n",
    "            root_id,\n",
    "            parent_id,\n",
    "            shadow,\n",
    "            chain,\n",
    "            ignore_result=ignore_result,\n",
    "            replaced_task_nesting=replaced_task_nesting,\n",
    "            **options,\n",
    "        )\n",
    "\n",
    "        stamped_headers = options.pop(\"stamped_headers\", [])\n",
    "        for stamp in stamped_headers:\n",
    "            options.pop(stamp)\n",
    "\n",
    "        if connection:\n",
    "            producer = amqp.Producer(connection, auto_declare=False)\n",
    "\n",
    "        with self.producer_or_acquire(producer) as P:\n",
    "            with P.connection._reraise_as_library_errors():\n",
    "                if not ignore_result:\n",
    "                    self.backend.on_task_call(P, task_id)\n",
    "                amqp.send_task_message(P, name, message, **options)\n",
    "        result = (result_cls or self.AsyncResult)(task_id)\n",
    "        # We avoid using the constructor since a custom result class\n",
    "        # can be used, in which case the constructor may still use\n",
    "        # the old signature.\n",
    "        result.ignored = ignore_result\n",
    "\n",
    "        if add_to_parent:\n",
    "            if not have_parent:\n",
    "                parent, have_parent = self.current_worker_task, True\n",
    "            if parent:\n",
    "                parent.add_trail(result)\n",
    "        return result\n",
    "\n",
    "    def connection_for_read(self, url=None, **kwargs):\n",
    "        \"\"\"Establish connection used for consuming.\n",
    "\n",
    "        See Also:\n",
    "            :meth:`connection` for supported arguments.\n",
    "        \"\"\"\n",
    "        return self._connection(url or self.conf.broker_read_url, **kwargs)\n",
    "\n",
    "    def connection_for_write(self, url=None, **kwargs):\n",
    "        \"\"\"Establish connection used for producing.\n",
    "\n",
    "        See Also:\n",
    "            :meth:`connection` for supported arguments.\n",
    "        \"\"\"\n",
    "        return self._connection(url or self.conf.broker_write_url, **kwargs)\n",
    "\n",
    "    def connection(\n",
    "        self,\n",
    "        hostname=None,\n",
    "        userid=None,\n",
    "        password=None,\n",
    "        virtual_host=None,\n",
    "        port=None,\n",
    "        ssl=None,\n",
    "        connect_timeout=None,\n",
    "        transport=None,\n",
    "        transport_options=None,\n",
    "        heartbeat=None,\n",
    "        login_method=None,\n",
    "        failover_strategy=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Establish a connection to the message broker.\n",
    "\n",
    "        Please use :meth:`connection_for_read` and\n",
    "        :meth:`connection_for_write` instead, to convey the intent\n",
    "        of use for this connection.\n",
    "\n",
    "        Arguments:\n",
    "            url: Either the URL or the hostname of the broker to use.\n",
    "            hostname (str): URL, Hostname/IP-address of the broker.\n",
    "                If a URL is used, then the other argument below will\n",
    "                be taken from the URL instead.\n",
    "            userid (str): Username to authenticate as.\n",
    "            password (str): Password to authenticate with\n",
    "            virtual_host (str): Virtual host to use (domain).\n",
    "            port (int): Port to connect to.\n",
    "            ssl (bool, Dict): Defaults to the :setting:`broker_use_ssl`\n",
    "                setting.\n",
    "            transport (str): defaults to the :setting:`broker_transport`\n",
    "                setting.\n",
    "            transport_options (Dict): Dictionary of transport specific options.\n",
    "            heartbeat (int): AMQP Heartbeat in seconds (``pyamqp`` only).\n",
    "            login_method (str): Custom login method to use (AMQP only).\n",
    "            failover_strategy (str, Callable): Custom failover strategy.\n",
    "            **kwargs: Additional arguments to :class:`kombu.Connection`.\n",
    "\n",
    "        Returns:\n",
    "            kombu.Connection: the lazy connection instance.\n",
    "        \"\"\"\n",
    "        return self.connection_for_write(\n",
    "            hostname or self.conf.broker_write_url,\n",
    "            userid=userid,\n",
    "            password=password,\n",
    "            virtual_host=virtual_host,\n",
    "            port=port,\n",
    "            ssl=ssl,\n",
    "            connect_timeout=connect_timeout,\n",
    "            transport=transport,\n",
    "            transport_options=transport_options,\n",
    "            heartbeat=heartbeat,\n",
    "            login_method=login_method,\n",
    "            failover_strategy=failover_strategy,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def _connection(\n",
    "        self,\n",
    "        url,\n",
    "        userid=None,\n",
    "        password=None,\n",
    "        virtual_host=None,\n",
    "        port=None,\n",
    "        ssl=None,\n",
    "        connect_timeout=None,\n",
    "        transport=None,\n",
    "        transport_options=None,\n",
    "        heartbeat=None,\n",
    "        login_method=None,\n",
    "        failover_strategy=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        conf = self.conf\n",
    "        return self.amqp.Connection(\n",
    "            url,\n",
    "            userid or conf.broker_user,\n",
    "            password or conf.broker_password,\n",
    "            virtual_host or conf.broker_vhost,\n",
    "            port or conf.broker_port,\n",
    "            transport=transport or conf.broker_transport,\n",
    "            ssl=self.either(\"broker_use_ssl\", ssl),\n",
    "            heartbeat=heartbeat,\n",
    "            login_method=login_method or conf.broker_login_method,\n",
    "            failover_strategy=(failover_strategy or conf.broker_failover_strategy),\n",
    "            transport_options=dict(\n",
    "                conf.broker_transport_options, **transport_options or {}\n",
    "            ),\n",
    "            connect_timeout=self.either(\"broker_connection_timeout\", connect_timeout),\n",
    "        )\n",
    "\n",
    "    broker_connection = connection\n",
    "\n",
    "    def _acquire_connection(self, pool=True):\n",
    "        \"\"\"Helper for :meth:`connection_or_acquire`.\"\"\"\n",
    "        if pool:\n",
    "            return self.pool.acquire(block=True)\n",
    "        return self.connection_for_write()\n",
    "\n",
    "    def connection_or_acquire(self, connection=None, pool=True, *_, **__):\n",
    "        \"\"\"Context used to acquire a connection from the pool.\n",
    "\n",
    "        For use within a :keyword:`with` statement to get a connection\n",
    "        from the pool if one is not already provided.\n",
    "\n",
    "        Arguments:\n",
    "            connection (kombu.Connection): If not provided, a connection\n",
    "                will be acquired from the connection pool.\n",
    "        \"\"\"\n",
    "        return FallbackContext(connection, self._acquire_connection, pool=pool)\n",
    "\n",
    "    default_connection = connection_or_acquire  # XXX compat\n",
    "\n",
    "    def producer_or_acquire(self, producer=None):\n",
    "        \"\"\"Context used to acquire a producer from the pool.\n",
    "\n",
    "        For use within a :keyword:`with` statement to get a producer\n",
    "        from the pool if one is not already provided\n",
    "\n",
    "        Arguments:\n",
    "            producer (kombu.Producer): If not provided, a producer\n",
    "                will be acquired from the producer pool.\n",
    "        \"\"\"\n",
    "        return FallbackContext(\n",
    "            producer,\n",
    "            self.producer_pool.acquire,\n",
    "            block=True,\n",
    "        )\n",
    "\n",
    "    default_producer = producer_or_acquire  # XXX compat\n",
    "\n",
    "    def prepare_config(self, c):\n",
    "        \"\"\"Prepare configuration before it is merged with the defaults.\"\"\"\n",
    "        return find_deprecated_settings(c)\n",
    "\n",
    "    def now(self):\n",
    "        \"\"\"Return the current time and date as a datetime.\"\"\"\n",
    "        now_in_utc = to_utc(datetime.now(datetime_timezone.utc))\n",
    "        return now_in_utc.astimezone(self.timezone)\n",
    "\n",
    "    def select_queues(self, queues=None):\n",
    "        \"\"\"Select subset of queues.\n",
    "\n",
    "        Arguments:\n",
    "            queues (Sequence[str]): a list of queue names to keep.\n",
    "        \"\"\"\n",
    "        return self.amqp.queues.select(queues)\n",
    "\n",
    "    def either(self, default_key, *defaults):\n",
    "        \"\"\"Get key from configuration or use default values.\n",
    "\n",
    "        Fallback to the value of a configuration key if none of the\n",
    "        `*values` are true.\n",
    "        \"\"\"\n",
    "        return first(\n",
    "            None,\n",
    "            [\n",
    "                first(None, defaults),\n",
    "                starpromise(self.conf.get, default_key),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def bugreport(self):\n",
    "        \"\"\"Return information useful in bug reports.\"\"\"\n",
    "        return bugreport(self)\n",
    "\n",
    "    def _get_backend(self):\n",
    "        backend, url = backends.by_url(\n",
    "            self.backend_cls or self.conf.result_backend, self.loader\n",
    "        )\n",
    "        return backend(app=self, url=url)\n",
    "\n",
    "    def _finalize_pending_conf(self):\n",
    "        \"\"\"Get config value by key and finalize loading the configuration.\n",
    "\n",
    "        Note:\n",
    "            This is used by PendingConfiguration:\n",
    "                as soon as you access a key the configuration is read.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            conf = self._conf = self._load_config()\n",
    "        except AttributeError as err:\n",
    "            # AttributeError is not propagated, it is \"handled\" by\n",
    "            # PendingConfiguration parent class. This causes\n",
    "            # confusing RecursionError.\n",
    "            raise ModuleNotFoundError(*err.args) from err\n",
    "\n",
    "        return conf\n",
    "\n",
    "    def _load_config(self):\n",
    "        if isinstance(self.on_configure, Signal):\n",
    "            self.on_configure.send(sender=self)\n",
    "        else:\n",
    "            # used to be a method pre 4.0\n",
    "            self.on_configure()\n",
    "        if self._config_source:\n",
    "            self.loader.config_from_object(self._config_source)\n",
    "        self.configured = True\n",
    "        settings = detect_settings(\n",
    "            self.prepare_config(self.loader.conf),\n",
    "            self._preconf,\n",
    "            ignore_keys=self._preconf_set_by_auto,\n",
    "            prefix=self.namespace,\n",
    "        )\n",
    "        if self._conf is not None:\n",
    "            # replace in place, as someone may have referenced app.conf,\n",
    "            # done some changes, accessed a key, and then try to make more\n",
    "            # changes to the reference and not the finalized value.\n",
    "            self._conf.swap_with(settings)\n",
    "        else:\n",
    "            self._conf = settings\n",
    "\n",
    "        # load lazy config dict initializers.\n",
    "        pending_def = self._pending_defaults\n",
    "        while pending_def:\n",
    "            self._conf.add_defaults(maybe_evaluate(pending_def.popleft()()))\n",
    "\n",
    "        # load lazy periodic tasks\n",
    "        pending_beat = self._pending_periodic_tasks\n",
    "        while pending_beat:\n",
    "            periodic_task_args, periodic_task_kwargs = pending_beat.popleft()\n",
    "            self._add_periodic_task(*periodic_task_args, **periodic_task_kwargs)\n",
    "\n",
    "        self.on_after_configure.send(sender=self, source=self._conf)\n",
    "        return self._conf\n",
    "\n",
    "    def _after_fork(self):\n",
    "        self._pool = None\n",
    "        try:\n",
    "            self.__dict__[\"amqp\"]._producer_pool = None\n",
    "        except (AttributeError, KeyError):\n",
    "            pass\n",
    "        self.on_after_fork.send(sender=self)\n",
    "\n",
    "    def signature(self, *args, **kwargs):\n",
    "        \"\"\"Return a new :class:`~celery.Signature` bound to this app.\"\"\"\n",
    "        kwargs[\"app\"] = self\n",
    "        return self._canvas.signature(*args, **kwargs)\n",
    "\n",
    "    def add_periodic_task(self, schedule, sig, args=(), kwargs=(), name=None, **opts):\n",
    "        \"\"\"\n",
    "        Add a periodic task to beat schedule.\n",
    "\n",
    "        Celery beat store tasks based on `sig` or `name` if provided. Adding the\n",
    "        same signature twice make the second task override the first one. To\n",
    "        avoid the override, use distinct `name` for them.\n",
    "        \"\"\"\n",
    "        key, entry = self._sig_to_periodic_task_entry(\n",
    "            schedule, sig, args, kwargs, name, **opts\n",
    "        )\n",
    "        if self.configured:\n",
    "            self._add_periodic_task(key, entry, name=name)\n",
    "        else:\n",
    "            self._pending_periodic_tasks.append([(key, entry), {\"name\": name}])\n",
    "        return key\n",
    "\n",
    "    def _sig_to_periodic_task_entry(\n",
    "        self, schedule, sig, args=(), kwargs=None, name=None, **opts\n",
    "    ):\n",
    "        kwargs = {} if not kwargs else kwargs\n",
    "        sig = (\n",
    "            sig.clone(args, kwargs)\n",
    "            if isinstance(sig, abstract.CallableSignature)\n",
    "            else self.signature(sig.name, args, kwargs)\n",
    "        )\n",
    "        return name or repr(sig), {\n",
    "            \"schedule\": schedule,\n",
    "            \"task\": sig.name,\n",
    "            \"args\": sig.args,\n",
    "            \"kwargs\": sig.kwargs,\n",
    "            \"options\": dict(sig.options, **opts),\n",
    "        }\n",
    "\n",
    "    def _add_periodic_task(self, key, entry, name=None):\n",
    "        if name is None and key in self._conf.beat_schedule:\n",
    "            logger.warning(\n",
    "                f\"Periodic task key='{key}' shadowed a previous unnamed periodic task.\"\n",
    "                \" Pass a name kwarg to add_periodic_task to silence this warning.\"\n",
    "            )\n",
    "\n",
    "        self._conf.beat_schedule[key] = entry\n",
    "\n",
    "    def create_task_cls(self):\n",
    "        \"\"\"Create a base task class bound to this app.\"\"\"\n",
    "        return self.subclass_with_self(\n",
    "            self.task_cls,\n",
    "            name=\"Task\",\n",
    "            attribute=\"_app\",\n",
    "            keep_reduce=True,\n",
    "            abstract=True,\n",
    "        )\n",
    "\n",
    "    def subclass_with_self(\n",
    "        self, Class, name=None, attribute=\"app\", reverse=None, keep_reduce=False, **kw\n",
    "    ):\n",
    "        \"\"\"Subclass an app-compatible class.\n",
    "\n",
    "        App-compatible means that the class has a class attribute that\n",
    "        provides the default app it should use, for example:\n",
    "        ``class Foo: app = None``.\n",
    "\n",
    "        Arguments:\n",
    "            Class (type): The app-compatible class to subclass.\n",
    "            name (str): Custom name for the target class.\n",
    "            attribute (str): Name of the attribute holding the app,\n",
    "                Default is 'app'.\n",
    "            reverse (str): Reverse path to this object used for pickling\n",
    "                purposes. For example, to get ``app.AsyncResult``,\n",
    "                use ``\"AsyncResult\"``.\n",
    "            keep_reduce (bool): If enabled a custom ``__reduce__``\n",
    "                implementation won't be provided.\n",
    "        \"\"\"\n",
    "        Class = symbol_by_name(Class)\n",
    "        reverse = reverse if reverse else Class.__name__\n",
    "\n",
    "        def __reduce__(self):\n",
    "            return _unpickle_appattr, (reverse, self.__reduce_args__())\n",
    "\n",
    "        attrs = dict(\n",
    "            {attribute: self}, __module__=Class.__module__, __doc__=Class.__doc__, **kw\n",
    "        )\n",
    "        if not keep_reduce:\n",
    "            attrs[\"__reduce__\"] = __reduce__\n",
    "\n",
    "        return type(name or Class.__name__, (Class,), attrs)\n",
    "\n",
    "    def _rgetattr(self, path):\n",
    "        return attrgetter(path)(self)\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *exc_info):\n",
    "        self.close()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"<{type(self).__name__} {appstr(self)}>\"\n",
    "\n",
    "    def __reduce__(self):\n",
    "        if self._using_v1_reduce:\n",
    "            return self.__reduce_v1__()\n",
    "        return (_unpickle_app_v2, (self.__class__, self.__reduce_keys__()))\n",
    "\n",
    "    def __reduce_v1__(self):\n",
    "        # Reduce only pickles the configuration changes,\n",
    "        # so the default configuration doesn't have to be passed\n",
    "        # between processes.\n",
    "        return (\n",
    "            _unpickle_app,\n",
    "            (self.__class__, self.Pickler) + self.__reduce_args__(),\n",
    "        )\n",
    "\n",
    "    def __reduce_keys__(self):\n",
    "        \"\"\"Keyword arguments used to reconstruct the object when unpickling.\"\"\"\n",
    "        return {\n",
    "            \"main\": self.main,\n",
    "            \"changes\": self._conf.changes if self.configured else self._preconf,\n",
    "            \"loader\": self.loader_cls,\n",
    "            \"backend\": self.backend_cls,\n",
    "            \"amqp\": self.amqp_cls,\n",
    "            \"events\": self.events_cls,\n",
    "            \"log\": self.log_cls,\n",
    "            \"control\": self.control_cls,\n",
    "            \"fixups\": self.fixups,\n",
    "            \"config_source\": self._config_source,\n",
    "            \"task_cls\": self.task_cls,\n",
    "            \"namespace\": self.namespace,\n",
    "        }\n",
    "\n",
    "    def __reduce_args__(self):\n",
    "        \"\"\"Deprecated method, please use :meth:`__reduce_keys__` instead.\"\"\"\n",
    "        return (\n",
    "            self.main,\n",
    "            self._conf.changes if self.configured else {},\n",
    "            self.loader_cls,\n",
    "            self.backend_cls,\n",
    "            self.amqp_cls,\n",
    "            self.events_cls,\n",
    "            self.log_cls,\n",
    "            self.control_cls,\n",
    "            False,\n",
    "            self._config_source,\n",
    "        )\n",
    "\n",
    "    @cached_property\n",
    "    def Worker(self):\n",
    "        \"\"\"Worker application.\n",
    "\n",
    "        See Also:\n",
    "            :class:`~@Worker`.\n",
    "        \"\"\"\n",
    "        return self.subclass_with_self(\"celery.apps.worker:Worker\")\n",
    "\n",
    "    @cached_property\n",
    "    def WorkController(self, **kwargs):\n",
    "        \"\"\"Embeddable worker.\n",
    "\n",
    "        See Also:\n",
    "            :class:`~@WorkController`.\n",
    "        \"\"\"\n",
    "        return self.subclass_with_self(\"celery.worker:WorkController\")\n",
    "\n",
    "    @cached_property\n",
    "    def Beat(self, **kwargs):\n",
    "        \"\"\":program:`celery beat` scheduler application.\n",
    "\n",
    "        See Also:\n",
    "            :class:`~@Beat`.\n",
    "        \"\"\"\n",
    "        return self.subclass_with_self(\"celery.apps.beat:Beat\")\n",
    "\n",
    "    @cached_property\n",
    "    def Task(self):\n",
    "        \"\"\"Base task class for this app.\"\"\"\n",
    "        return self.create_task_cls()\n",
    "\n",
    "    @cached_property\n",
    "    def annotations(self):\n",
    "        return prepare_annotations(self.conf.task_annotations)\n",
    "\n",
    "    @cached_property\n",
    "    def AsyncResult(self):\n",
    "        \"\"\"Create new result instance.\n",
    "\n",
    "        See Also:\n",
    "            :class:`celery.result.AsyncResult`.\n",
    "        \"\"\"\n",
    "        return self.subclass_with_self(\"celery.result:AsyncResult\")\n",
    "\n",
    "    @cached_property\n",
    "    def ResultSet(self):\n",
    "        return self.subclass_with_self(\"celery.result:ResultSet\")\n",
    "\n",
    "    @cached_property\n",
    "    def GroupResult(self):\n",
    "        \"\"\"Create new group result instance.\n",
    "\n",
    "        See Also:\n",
    "            :class:`celery.result.GroupResult`.\n",
    "        \"\"\"\n",
    "        return self.subclass_with_self(\"celery.result:GroupResult\")\n",
    "\n",
    "    @property\n",
    "    def pool(self):\n",
    "        \"\"\"Broker connection pool: :class:`~@pool`.\n",
    "\n",
    "        Note:\n",
    "            This attribute is not related to the workers concurrency pool.\n",
    "        \"\"\"\n",
    "        if self._pool is None:\n",
    "            self._ensure_after_fork()\n",
    "            limit = self.conf.broker_pool_limit\n",
    "            pools.set_limit(limit)\n",
    "            self._pool = pools.connections[self.connection_for_write()]\n",
    "        return self._pool\n",
    "\n",
    "    @property\n",
    "    def current_task(self):\n",
    "        \"\"\"Instance of task being executed, or :const:`None`.\"\"\"\n",
    "        return _task_stack.top\n",
    "\n",
    "    @property\n",
    "    def current_worker_task(self):\n",
    "        \"\"\"The task currently being executed by a worker or :const:`None`.\n",
    "\n",
    "        Differs from :data:`current_task` in that it's not affected\n",
    "        by tasks calling other tasks directly, or eagerly.\n",
    "        \"\"\"\n",
    "        return get_current_worker_task()\n",
    "\n",
    "    @cached_property\n",
    "    def oid(self):\n",
    "        \"\"\"Universally unique identifier for this app.\"\"\"\n",
    "        # since 4.0: thread.get_ident() is not included when\n",
    "        # generating the process id.  This is due to how the RPC\n",
    "        # backend now dedicates a single thread to receive results,\n",
    "        # which would not work if each thread has a separate id.\n",
    "        return oid_from(self, threads=False)\n",
    "\n",
    "    @property\n",
    "    def thread_oid(self):\n",
    "        \"\"\"Per-thread unique identifier for this app.\"\"\"\n",
    "        try:\n",
    "            return self._local.oid\n",
    "        except AttributeError:\n",
    "            self._local.oid = new_oid = oid_from(self, threads=True)\n",
    "            return new_oid\n",
    "\n",
    "    @cached_property\n",
    "    def amqp(self):\n",
    "        \"\"\"AMQP related functionality: :class:`~@amqp`.\"\"\"\n",
    "        return instantiate(self.amqp_cls, app=self)\n",
    "\n",
    "    @property\n",
    "    def _backend(self):\n",
    "        \"\"\"A reference to the backend object\n",
    "\n",
    "        Uses self._backend_cache if it is thread safe.\n",
    "        Otherwise, use self._local\n",
    "        \"\"\"\n",
    "        if self._backend_cache is not None:\n",
    "            return self._backend_cache\n",
    "        return getattr(self._local, \"backend\", None)\n",
    "\n",
    "    @_backend.setter\n",
    "    def _backend(self, backend):\n",
    "        \"\"\"Set the backend object on the app\"\"\"\n",
    "        if backend.thread_safe:\n",
    "            self._backend_cache = backend\n",
    "        else:\n",
    "            self._local.backend = backend\n",
    "\n",
    "    @property\n",
    "    def backend(self):\n",
    "        \"\"\"Current backend instance.\"\"\"\n",
    "        if self._backend is None:\n",
    "            self._backend = self._get_backend()\n",
    "        return self._backend\n",
    "\n",
    "    @property\n",
    "    def conf(self):\n",
    "        \"\"\"Current configuration.\"\"\"\n",
    "        if self._conf is None:\n",
    "            self._conf = self._load_config()\n",
    "        return self._conf\n",
    "\n",
    "    @conf.setter\n",
    "    def conf(self, d):\n",
    "        self._conf = d\n",
    "\n",
    "    @cached_property\n",
    "    def control(self):\n",
    "        \"\"\"Remote control: :class:`~@control`.\"\"\"\n",
    "        return instantiate(self.control_cls, app=self)\n",
    "\n",
    "    @cached_property\n",
    "    def events(self):\n",
    "        \"\"\"Consuming and sending events: :class:`~@events`.\"\"\"\n",
    "        return instantiate(self.events_cls, app=self)\n",
    "\n",
    "    @cached_property\n",
    "    def loader(self):\n",
    "        \"\"\"Current loader instance.\"\"\"\n",
    "        return get_loader_cls(self.loader_cls)(app=self)\n",
    "\n",
    "    @cached_property\n",
    "    def log(self):\n",
    "        \"\"\"Logging: :class:`~@log`.\"\"\"\n",
    "        return instantiate(self.log_cls, app=self)\n",
    "\n",
    "    @cached_property\n",
    "    def _canvas(self):\n",
    "        from celery import canvas\n",
    "\n",
    "        return canvas\n",
    "\n",
    "    @cached_property\n",
    "    def tasks(self):\n",
    "        \"\"\"Task registry.\n",
    "\n",
    "        Warning:\n",
    "            Accessing this attribute will also auto-finalize the app.\n",
    "        \"\"\"\n",
    "        self.finalize(auto=True)\n",
    "        return self._tasks\n",
    "\n",
    "    @property\n",
    "    def producer_pool(self):\n",
    "        return self.amqp.producer_pool\n",
    "\n",
    "    def uses_utc_timezone(self):\n",
    "        \"\"\"Check if the application uses the UTC timezone.\"\"\"\n",
    "        return self.timezone == timezone.utc\n",
    "\n",
    "    @cached_property\n",
    "    def timezone(self):\n",
    "        \"\"\"Current timezone for this app.\n",
    "\n",
    "        This is a cached property taking the time zone from the\n",
    "        :setting:`timezone` setting.\n",
    "        \"\"\"\n",
    "        conf = self.conf\n",
    "        if not conf.timezone:\n",
    "            if conf.enable_utc:\n",
    "                return timezone.utc\n",
    "            else:\n",
    "                return timezone.local\n",
    "        return timezone.get_timezone(conf.timezone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "App = Celery  # XXX compat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}