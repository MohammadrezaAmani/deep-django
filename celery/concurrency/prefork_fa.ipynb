{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Prefork execution pool.\n",
    "\n",
    "Pool implementation using :mod:`multiprocessing`.\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "from billiard import forking_enable\n",
    "from billiard.common import REMAP_SIGTERM, TERM_SIGNAME\n",
    "from billiard.pool import CLOSE, RUN\n",
    "from billiard.pool import Pool as BlockingPool\n",
    "\n",
    "from celery import platforms, signals\n",
    "from celery._state import _set_task_join_will_block, set_default_app\n",
    "from celery.app import trace\n",
    "from celery.concurrency.base import BasePool\n",
    "from celery.utils.functional import noop\n",
    "from celery.utils.log import get_logger\n",
    "\n",
    "from .asynpool import AsynPool\n",
    "\n",
    "__all__ = (\"TaskPool\", \"process_initializer\", \"process_destructor\")\n",
    "\n",
    "#: List of signals to reset when a child process starts.\n",
    "WORKER_SIGRESET = {\n",
    "    \"SIGTERM\",\n",
    "    \"SIGHUP\",\n",
    "    \"SIGTTIN\",\n",
    "    \"SIGTTOU\",\n",
    "    \"SIGUSR1\",\n",
    "}\n",
    "\n",
    "#: List of signals to ignore when a child process starts.\n",
    "if REMAP_SIGTERM:\n",
    "    WORKER_SIGIGNORE = {\"SIGINT\", TERM_SIGNAME}\n",
    "else:\n",
    "    WORKER_SIGIGNORE = {\"SIGINT\"}\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "warning, debug = logger.warning, logger.debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_initializer(app, hostname):\n",
    "    \"\"\"Pool child process initializer.\n",
    "\n",
    "    Initialize the child pool process to ensure the correct\n",
    "    app instance is used and things like logging works.\n",
    "    \"\"\"\n",
    "    # Each running worker gets SIGKILL by OS when main process exits.\n",
    "    platforms.set_pdeathsig(\"SIGKILL\")\n",
    "    _set_task_join_will_block(True)\n",
    "    platforms.signals.reset(*WORKER_SIGRESET)\n",
    "    platforms.signals.ignore(*WORKER_SIGIGNORE)\n",
    "    platforms.set_mp_process_title(\"celeryd\", hostname=hostname)\n",
    "    # This is for Windows and other platforms not supporting\n",
    "    # fork().  Note that init_worker makes sure it's only\n",
    "    # run once per process.\n",
    "    app.loader.init_worker()\n",
    "    app.loader.init_worker_process()\n",
    "    logfile = os.environ.get(\"CELERY_LOG_FILE\") or None\n",
    "    if logfile and \"%i\" in logfile.lower():\n",
    "        # logfile path will differ so need to set up logging again.\n",
    "        app.log.already_setup = False\n",
    "    app.log.setup(\n",
    "        int(os.environ.get(\"CELERY_LOG_LEVEL\", 0) or 0),\n",
    "        logfile,\n",
    "        bool(os.environ.get(\"CELERY_LOG_REDIRECT\", False)),\n",
    "        str(os.environ.get(\"CELERY_LOG_REDIRECT_LEVEL\")),\n",
    "        hostname=hostname,\n",
    "    )\n",
    "    if os.environ.get(\"FORKED_BY_MULTIPROCESSING\"):\n",
    "        # pool did execv after fork\n",
    "        trace.setup_worker_optimizations(app, hostname)\n",
    "    else:\n",
    "        app.set_current()\n",
    "        set_default_app(app)\n",
    "        app.finalize()\n",
    "        trace._tasks = app._tasks  # enables fast_trace_task optimization.\n",
    "    # rebuild execution handler for all tasks.\n",
    "    from celery.app.trace import build_tracer\n",
    "\n",
    "    for name, task in app.tasks.items():\n",
    "        task.__trace__ = build_tracer(name, task, app.loader, hostname, app=app)\n",
    "    from celery.worker import state as worker_state\n",
    "\n",
    "    worker_state.reset_state()\n",
    "    signals.worker_process_init.send(sender=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_destructor(pid, exitcode):\n",
    "    \"\"\"Pool child process destructor.\n",
    "\n",
    "    Dispatch the :signal:`worker_process_shutdown` signal.\n",
    "    \"\"\"\n",
    "    signals.worker_process_shutdown.send(\n",
    "        sender=None,\n",
    "        pid=pid,\n",
    "        exitcode=exitcode,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskPool(BasePool):\n",
    "    \"\"\"Multiprocessing Pool implementation.\"\"\"\n",
    "\n",
    "    Pool = AsynPool\n",
    "    BlockingPool = BlockingPool\n",
    "\n",
    "    uses_semaphore = True\n",
    "    write_stats = None\n",
    "\n",
    "    def on_start(self):\n",
    "        forking_enable(self.forking_enable)\n",
    "        Pool = self.BlockingPool if self.options.get(\"threads\", True) else self.Pool\n",
    "        proc_alive_timeout = (\n",
    "            self.app.conf.worker_proc_alive_timeout if self.app else None\n",
    "        )\n",
    "        P = self._pool = Pool(\n",
    "            processes=self.limit,\n",
    "            initializer=process_initializer,\n",
    "            on_process_exit=process_destructor,\n",
    "            enable_timeouts=True,\n",
    "            synack=False,\n",
    "            proc_alive_timeout=proc_alive_timeout,\n",
    "            **self.options\n",
    "        )\n",
    "\n",
    "        # Create proxy methods\n",
    "        self.on_apply = P.apply_async\n",
    "        self.maintain_pool = P.maintain_pool\n",
    "        self.terminate_job = P.terminate_job\n",
    "        self.grow = P.grow\n",
    "        self.shrink = P.shrink\n",
    "        self.flush = getattr(P, \"flush\", None)  # FIXME add to billiard\n",
    "\n",
    "    def restart(self):\n",
    "        self._pool.restart()\n",
    "        self._pool.apply_async(noop)\n",
    "\n",
    "    def did_start_ok(self):\n",
    "        return self._pool.did_start_ok()\n",
    "\n",
    "    def register_with_event_loop(self, loop):\n",
    "        try:\n",
    "            reg = self._pool.register_with_event_loop\n",
    "        except AttributeError:\n",
    "            return\n",
    "        return reg(loop)\n",
    "\n",
    "    def on_stop(self):\n",
    "        \"\"\"Gracefully stop the pool.\"\"\"\n",
    "        if self._pool is not None and self._pool._state in (RUN, CLOSE):\n",
    "            self._pool.close()\n",
    "            self._pool.join()\n",
    "            self._pool = None\n",
    "\n",
    "    def on_terminate(self):\n",
    "        \"\"\"Force terminate the pool.\"\"\"\n",
    "        if self._pool is not None:\n",
    "            self._pool.terminate()\n",
    "            self._pool = None\n",
    "\n",
    "    def on_close(self):\n",
    "        if self._pool is not None and self._pool._state == RUN:\n",
    "            self._pool.close()\n",
    "\n",
    "    def _get_info(self):\n",
    "        write_stats = getattr(self._pool, \"human_write_stats\", None)\n",
    "        info = super()._get_info()\n",
    "        info.update(\n",
    "            {\n",
    "                \"max-concurrency\": self.limit,\n",
    "                \"processes\": [p.pid for p in self._pool._pool],\n",
    "                \"max-tasks-per-child\": self._pool._maxtasksperchild or \"N/A\",\n",
    "                \"put-guarded-by-semaphore\": self.putlocks,\n",
    "                \"timeouts\": (self._pool.soft_timeout or 0, self._pool.timeout or 0),\n",
    "                \"writes\": write_stats() if write_stats is not None else \"N/A\",\n",
    "            }\n",
    "        )\n",
    "        return info\n",
    "\n",
    "    @property\n",
    "    def num_processes(self):\n",
    "        return self._pool._processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}