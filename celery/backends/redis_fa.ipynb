{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Redis result store backend.\"\"\"\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from functools import partial\n",
    "from ssl import CERT_NONE, CERT_OPTIONAL, CERT_REQUIRED\n",
    "from urllib.parse import unquote\n",
    "\n",
    "from kombu.utils.functional import retry_over_time\n",
    "from kombu.utils.objects import cached_property\n",
    "from kombu.utils.url import _parse_url, maybe_sanitize_url\n",
    "\n",
    "from celery import states\n",
    "from celery._state import task_join_will_block\n",
    "from celery.canvas import maybe_signature\n",
    "from celery.exceptions import BackendStoreError, ChordError, ImproperlyConfigured\n",
    "from celery.result import GroupResult, allow_join_result\n",
    "from celery.utils.functional import _regen, dictfilter\n",
    "from celery.utils.log import get_logger\n",
    "from celery.utils.time import humanize_seconds\n",
    "\n",
    "from .asynchronous import AsyncBackendMixin, BaseResultConsumer\n",
    "from .base import BaseKeyValueStoreBackend\n",
    "\n",
    "try:\n",
    "    import redis.connection\n",
    "    from kombu.transport.redis import get_redis_error_classes\n",
    "except ImportError:\n",
    "    redis = None\n",
    "    get_redis_error_classes = None\n",
    "\n",
    "try:\n",
    "    import redis.sentinel\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "__all__ = (\"RedisBackend\", \"SentinelBackend\")\n",
    "\n",
    "E_REDIS_MISSING = \"\"\"\n",
    "You need to install the redis library in order to use \\\n",
    "the Redis result store backend.\n",
    "\"\"\"\n",
    "\n",
    "E_REDIS_SENTINEL_MISSING = \"\"\"\n",
    "You need to install the redis library with support of \\\n",
    "sentinel in order to use the Redis result store backend.\n",
    "\"\"\"\n",
    "\n",
    "W_REDIS_SSL_CERT_OPTIONAL = \"\"\"\n",
    "Setting ssl_cert_reqs=CERT_OPTIONAL when connecting to redis means that \\\n",
    "celery might not validate the identity of the redis broker when connecting. \\\n",
    "This leaves you vulnerable to man in the middle attacks.\n",
    "\"\"\"\n",
    "\n",
    "W_REDIS_SSL_CERT_NONE = \"\"\"\n",
    "Setting ssl_cert_reqs=CERT_NONE when connecting to redis means that celery \\\n",
    "will not validate the identity of the redis broker when connecting. This \\\n",
    "leaves you vulnerable to man in the middle attacks.\n",
    "\"\"\"\n",
    "\n",
    "E_REDIS_SSL_PARAMS_AND_SCHEME_MISMATCH = \"\"\"\n",
    "SSL connection parameters have been provided but the specified URL scheme \\\n",
    "is redis://. A Redis SSL connection URL should use the scheme rediss://.\n",
    "\"\"\"\n",
    "\n",
    "E_REDIS_SSL_CERT_REQS_MISSING_INVALID = \"\"\"\n",
    "A rediss:// URL must have parameter ssl_cert_reqs and this must be set to \\\n",
    "CERT_REQUIRED, CERT_OPTIONAL, or CERT_NONE\n",
    "\"\"\"\n",
    "\n",
    "E_LOST = \"Connection to Redis lost: Retry (%s/%s) %s.\"\n",
    "\n",
    "E_RETRY_LIMIT_EXCEEDED = \"\"\"\n",
    "Retry limit exceeded while trying to reconnect to the Celery redis result \\\n",
    "store backend. The Celery application must be restarted.\n",
    "\"\"\"\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultConsumer(BaseResultConsumer):\n",
    "    _pubsub = None\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._get_key_for_task = self.backend.get_key_for_task\n",
    "        self._decode_result = self.backend.decode_result\n",
    "        self._ensure = self.backend.ensure\n",
    "        self._connection_errors = self.backend.connection_errors\n",
    "        self.subscribed_to = set()\n",
    "\n",
    "    def on_after_fork(self):\n",
    "        try:\n",
    "            self.backend.client.connection_pool.reset()\n",
    "            if self._pubsub is not None:\n",
    "                self._pubsub.close()\n",
    "        except KeyError as e:\n",
    "            logger.warning(str(e))\n",
    "        super().on_after_fork()\n",
    "\n",
    "    def _reconnect_pubsub(self):\n",
    "        self._pubsub = None\n",
    "        self.backend.client.connection_pool.reset()\n",
    "        # task state might have changed when the connection was down so we\n",
    "        # retrieve meta for all subscribed tasks before going into pubsub mode\n",
    "        if self.subscribed_to:\n",
    "            metas = self.backend.client.mget(self.subscribed_to)\n",
    "            metas = [meta for meta in metas if meta]\n",
    "            for meta in metas:\n",
    "                self.on_state_change(self._decode_result(meta), None)\n",
    "        self._pubsub = self.backend.client.pubsub(\n",
    "            ignore_subscribe_messages=True,\n",
    "        )\n",
    "        # subscribed_to maybe empty after on_state_change\n",
    "        if self.subscribed_to:\n",
    "            self._pubsub.subscribe(*self.subscribed_to)\n",
    "        else:\n",
    "            self._pubsub.connection = self._pubsub.connection_pool.get_connection(\n",
    "                \"pubsub\", self._pubsub.shard_hint\n",
    "            )\n",
    "            # even if there is nothing to subscribe, we should not lose the callback after connecting.\n",
    "            # The on_connect callback will re-subscribe to any channels we previously subscribed to.\n",
    "            self._pubsub.connection.register_connect_callback(self._pubsub.on_connect)\n",
    "\n",
    "    @contextmanager\n",
    "    def reconnect_on_error(self):\n",
    "        try:\n",
    "            yield\n",
    "        except self._connection_errors:\n",
    "            try:\n",
    "                self._ensure(self._reconnect_pubsub, ())\n",
    "            except self._connection_errors:\n",
    "                logger.critical(E_RETRY_LIMIT_EXCEEDED)\n",
    "                raise\n",
    "\n",
    "    def _maybe_cancel_ready_task(self, meta):\n",
    "        if meta[\"status\"] in states.READY_STATES:\n",
    "            self.cancel_for(meta[\"task_id\"])\n",
    "\n",
    "    def on_state_change(self, meta, message):\n",
    "        super().on_state_change(meta, message)\n",
    "        self._maybe_cancel_ready_task(meta)\n",
    "\n",
    "    def start(self, initial_task_id, **kwargs):\n",
    "        self._pubsub = self.backend.client.pubsub(\n",
    "            ignore_subscribe_messages=True,\n",
    "        )\n",
    "        self._consume_from(initial_task_id)\n",
    "\n",
    "    def on_wait_for_pending(self, result, **kwargs):\n",
    "        for meta in result._iter_meta(**kwargs):\n",
    "            if meta is not None:\n",
    "                self.on_state_change(meta, None)\n",
    "\n",
    "    def stop(self):\n",
    "        if self._pubsub is not None:\n",
    "            self._pubsub.close()\n",
    "\n",
    "    def drain_events(self, timeout=None):\n",
    "        if self._pubsub:\n",
    "            with self.reconnect_on_error():\n",
    "                message = self._pubsub.get_message(timeout=timeout)\n",
    "                if message and message[\"type\"] == \"message\":\n",
    "                    self.on_state_change(self._decode_result(message[\"data\"]), message)\n",
    "        elif timeout:\n",
    "            time.sleep(timeout)\n",
    "\n",
    "    def consume_from(self, task_id):\n",
    "        if self._pubsub is None:\n",
    "            return self.start(task_id)\n",
    "        self._consume_from(task_id)\n",
    "\n",
    "    def _consume_from(self, task_id):\n",
    "        key = self._get_key_for_task(task_id)\n",
    "        if key not in self.subscribed_to:\n",
    "            self.subscribed_to.add(key)\n",
    "            with self.reconnect_on_error():\n",
    "                self._pubsub.subscribe(key)\n",
    "\n",
    "    def cancel_for(self, task_id):\n",
    "        key = self._get_key_for_task(task_id)\n",
    "        self.subscribed_to.discard(key)\n",
    "        if self._pubsub:\n",
    "            with self.reconnect_on_error():\n",
    "                self._pubsub.unsubscribe(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedisBackend(BaseKeyValueStoreBackend, AsyncBackendMixin):\n",
    "    \"\"\"Redis task result store.\n",
    "\n",
    "    It makes use of the following commands:\n",
    "    GET, MGET, DEL, INCRBY, EXPIRE, SET, SETEX\n",
    "    \"\"\"\n",
    "\n",
    "    ResultConsumer = ResultConsumer\n",
    "\n",
    "    #: :pypi:`redis` client module.\n",
    "    redis = redis\n",
    "    connection_class_ssl = redis.SSLConnection if redis else None\n",
    "\n",
    "    #: Maximum number of connections in the pool.\n",
    "    max_connections = None\n",
    "\n",
    "    supports_autoexpire = True\n",
    "    supports_native_join = True\n",
    "\n",
    "    #: Maximal length of string value in Redis.\n",
    "    #: 512 MB - https://redis.io/topics/data-types\n",
    "    _MAX_STR_VALUE_SIZE = 536870912\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        host=None,\n",
    "        port=None,\n",
    "        db=None,\n",
    "        password=None,\n",
    "        max_connections=None,\n",
    "        url=None,\n",
    "        connection_pool=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(expires_type=int, **kwargs)\n",
    "        _get = self.app.conf.get\n",
    "        if self.redis is None:\n",
    "            raise ImproperlyConfigured(E_REDIS_MISSING.strip())\n",
    "\n",
    "        if host and \"://\" in host:\n",
    "            url, host = host, None\n",
    "\n",
    "        self.max_connections = (\n",
    "            max_connections or _get(\"redis_max_connections\") or self.max_connections\n",
    "        )\n",
    "        self._ConnectionPool = connection_pool\n",
    "\n",
    "        socket_timeout = _get(\"redis_socket_timeout\")\n",
    "        socket_connect_timeout = _get(\"redis_socket_connect_timeout\")\n",
    "        retry_on_timeout = _get(\"redis_retry_on_timeout\")\n",
    "        socket_keepalive = _get(\"redis_socket_keepalive\")\n",
    "        health_check_interval = _get(\"redis_backend_health_check_interval\")\n",
    "\n",
    "        self.connparams = {\n",
    "            \"host\": _get(\"redis_host\") or \"localhost\",\n",
    "            \"port\": _get(\"redis_port\") or 6379,\n",
    "            \"db\": _get(\"redis_db\") or 0,\n",
    "            \"password\": _get(\"redis_password\"),\n",
    "            \"max_connections\": self.max_connections,\n",
    "            \"socket_timeout\": socket_timeout and float(socket_timeout),\n",
    "            \"retry_on_timeout\": retry_on_timeout or False,\n",
    "            \"socket_connect_timeout\": socket_connect_timeout\n",
    "            and float(socket_connect_timeout),\n",
    "        }\n",
    "\n",
    "        username = _get(\"redis_username\")\n",
    "        if username:\n",
    "            # We're extra careful to avoid including this configuration value\n",
    "            # if it wasn't specified since older versions of py-redis\n",
    "            # don't support specifying a username.\n",
    "            # Only Redis>6.0 supports username/password authentication.\n",
    "\n",
    "            # TODO: Include this in connparams' definition once we drop\n",
    "            #       support for py-redis<3.4.0.\n",
    "            self.connparams[\"username\"] = username\n",
    "\n",
    "        if health_check_interval:\n",
    "            self.connparams[\"health_check_interval\"] = health_check_interval\n",
    "\n",
    "        # absent in redis.connection.UnixDomainSocketConnection\n",
    "        if socket_keepalive:\n",
    "            self.connparams[\"socket_keepalive\"] = socket_keepalive\n",
    "\n",
    "        # \"redis_backend_use_ssl\" must be a dict with the keys:\n",
    "        # 'ssl_cert_reqs', 'ssl_ca_certs', 'ssl_certfile', 'ssl_keyfile'\n",
    "        # (the same as \"broker_use_ssl\")\n",
    "        ssl = _get(\"redis_backend_use_ssl\")\n",
    "        if ssl:\n",
    "            self.connparams.update(ssl)\n",
    "            self.connparams[\"connection_class\"] = self.connection_class_ssl\n",
    "\n",
    "        if url:\n",
    "            self.connparams = self._params_from_url(url, self.connparams)\n",
    "\n",
    "        # If we've received SSL parameters via query string or the\n",
    "        # redis_backend_use_ssl dict, check ssl_cert_reqs is valid. If set\n",
    "        # via query string ssl_cert_reqs will be a string so convert it here\n",
    "        if \"connection_class\" in self.connparams and issubclass(\n",
    "            self.connparams[\"connection_class\"], redis.SSLConnection\n",
    "        ):\n",
    "            ssl_cert_reqs_missing = \"MISSING\"\n",
    "            ssl_string_to_constant = {\n",
    "                \"CERT_REQUIRED\": CERT_REQUIRED,\n",
    "                \"CERT_OPTIONAL\": CERT_OPTIONAL,\n",
    "                \"CERT_NONE\": CERT_NONE,\n",
    "                \"required\": CERT_REQUIRED,\n",
    "                \"optional\": CERT_OPTIONAL,\n",
    "                \"none\": CERT_NONE,\n",
    "            }\n",
    "            ssl_cert_reqs = self.connparams.get(\"ssl_cert_reqs\", ssl_cert_reqs_missing)\n",
    "            ssl_cert_reqs = ssl_string_to_constant.get(ssl_cert_reqs, ssl_cert_reqs)\n",
    "            if ssl_cert_reqs not in ssl_string_to_constant.values():\n",
    "                raise ValueError(E_REDIS_SSL_CERT_REQS_MISSING_INVALID)\n",
    "\n",
    "            if ssl_cert_reqs == CERT_OPTIONAL:\n",
    "                logger.warning(W_REDIS_SSL_CERT_OPTIONAL)\n",
    "            elif ssl_cert_reqs == CERT_NONE:\n",
    "                logger.warning(W_REDIS_SSL_CERT_NONE)\n",
    "            self.connparams[\"ssl_cert_reqs\"] = ssl_cert_reqs\n",
    "\n",
    "        self.url = url\n",
    "\n",
    "        self.connection_errors, self.channel_errors = (\n",
    "            get_redis_error_classes() if get_redis_error_classes else ((), ())\n",
    "        )\n",
    "        self.result_consumer = self.ResultConsumer(\n",
    "            self,\n",
    "            self.app,\n",
    "            self.accept,\n",
    "            self._pending_results,\n",
    "            self._pending_messages,\n",
    "        )\n",
    "\n",
    "    def _params_from_url(self, url, defaults):\n",
    "        scheme, host, port, username, password, path, query = _parse_url(url)\n",
    "        connparams = dict(\n",
    "            defaults,\n",
    "            **dictfilter(\n",
    "                {\n",
    "                    \"host\": host,\n",
    "                    \"port\": port,\n",
    "                    \"username\": username,\n",
    "                    \"password\": password,\n",
    "                    \"db\": query.pop(\"virtual_host\", None),\n",
    "                }\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        if scheme == \"socket\":\n",
    "            # use 'path' as path to the socket… in this case\n",
    "            # the database number should be given in 'query'\n",
    "            connparams.update(\n",
    "                {\n",
    "                    \"connection_class\": self.redis.UnixDomainSocketConnection,\n",
    "                    \"path\": \"/\" + path,\n",
    "                }\n",
    "            )\n",
    "            # host+port are invalid options when using this connection type.\n",
    "            connparams.pop(\"host\", None)\n",
    "            connparams.pop(\"port\", None)\n",
    "            connparams.pop(\"socket_connect_timeout\")\n",
    "        else:\n",
    "            connparams[\"db\"] = path\n",
    "\n",
    "        ssl_param_keys = [\n",
    "            \"ssl_ca_certs\",\n",
    "            \"ssl_certfile\",\n",
    "            \"ssl_keyfile\",\n",
    "            \"ssl_cert_reqs\",\n",
    "        ]\n",
    "\n",
    "        if scheme == \"redis\":\n",
    "            # If connparams or query string contain ssl params, raise error\n",
    "            if any(key in connparams for key in ssl_param_keys) or any(\n",
    "                key in query for key in ssl_param_keys\n",
    "            ):\n",
    "                raise ValueError(E_REDIS_SSL_PARAMS_AND_SCHEME_MISMATCH)\n",
    "\n",
    "        if scheme == \"rediss\":\n",
    "            connparams[\"connection_class\"] = redis.SSLConnection\n",
    "            # The following parameters, if present in the URL, are encoded. We\n",
    "            # must add the decoded values to connparams.\n",
    "            for ssl_setting in ssl_param_keys:\n",
    "                ssl_val = query.pop(ssl_setting, None)\n",
    "                if ssl_val:\n",
    "                    connparams[ssl_setting] = unquote(ssl_val)\n",
    "\n",
    "        # db may be string and start with / like in kombu.\n",
    "        db = connparams.get(\"db\") or 0\n",
    "        db = db.strip(\"/\") if isinstance(db, str) else db\n",
    "        connparams[\"db\"] = int(db)\n",
    "\n",
    "        for key, value in query.items():\n",
    "            if key in redis.connection.URL_QUERY_ARGUMENT_PARSERS:\n",
    "                query[key] = redis.connection.URL_QUERY_ARGUMENT_PARSERS[key](value)\n",
    "\n",
    "        # Query parameters override other parameters\n",
    "        connparams.update(query)\n",
    "        return connparams\n",
    "\n",
    "    @cached_property\n",
    "    def retry_policy(self):\n",
    "        retry_policy = super().retry_policy\n",
    "        if \"retry_policy\" in self._transport_options:\n",
    "            retry_policy = retry_policy.copy()\n",
    "            retry_policy.update(self._transport_options[\"retry_policy\"])\n",
    "\n",
    "        return retry_policy\n",
    "\n",
    "    def on_task_call(self, producer, task_id):\n",
    "        if not task_join_will_block():\n",
    "            self.result_consumer.consume_from(task_id)\n",
    "\n",
    "    def get(self, key):\n",
    "        return self.client.get(key)\n",
    "\n",
    "    def mget(self, keys):\n",
    "        return self.client.mget(keys)\n",
    "\n",
    "    def ensure(self, fun, args, **policy):\n",
    "        retry_policy = dict(self.retry_policy, **policy)\n",
    "        max_retries = retry_policy.get(\"max_retries\")\n",
    "        return retry_over_time(\n",
    "            fun,\n",
    "            self.connection_errors,\n",
    "            args,\n",
    "            {},\n",
    "            partial(self.on_connection_error, max_retries),\n",
    "            **retry_policy,\n",
    "        )\n",
    "\n",
    "    def on_connection_error(self, max_retries, exc, intervals, retries):\n",
    "        tts = next(intervals)\n",
    "        logger.error(\n",
    "            E_LOST.strip(), retries, max_retries or \"Inf\", humanize_seconds(tts, \"in \")\n",
    "        )\n",
    "        return tts\n",
    "\n",
    "    def set(self, key, value, **retry_policy):\n",
    "        if isinstance(value, str) and len(value) > self._MAX_STR_VALUE_SIZE:\n",
    "            raise BackendStoreError(\"value too large for Redis backend\")\n",
    "\n",
    "        return self.ensure(self._set, (key, value), **retry_policy)\n",
    "\n",
    "    def _set(self, key, value):\n",
    "        with self.client.pipeline() as pipe:\n",
    "            if self.expires:\n",
    "                pipe.setex(key, self.expires, value)\n",
    "            else:\n",
    "                pipe.set(key, value)\n",
    "            pipe.publish(key, value)\n",
    "            pipe.execute()\n",
    "\n",
    "    def forget(self, task_id):\n",
    "        super().forget(task_id)\n",
    "        self.result_consumer.cancel_for(task_id)\n",
    "\n",
    "    def delete(self, key):\n",
    "        self.client.delete(key)\n",
    "\n",
    "    def incr(self, key):\n",
    "        return self.client.incr(key)\n",
    "\n",
    "    def expire(self, key, value):\n",
    "        return self.client.expire(key, value)\n",
    "\n",
    "    def add_to_chord(self, group_id, result):\n",
    "        self.client.incr(self.get_key_for_group(group_id, \".t\"), 1)\n",
    "\n",
    "    def _unpack_chord_result(\n",
    "        self,\n",
    "        tup,\n",
    "        decode,\n",
    "        EXCEPTION_STATES=states.EXCEPTION_STATES,\n",
    "        PROPAGATE_STATES=states.PROPAGATE_STATES,\n",
    "    ):\n",
    "        _, tid, state, retval = decode(tup)\n",
    "        if state in EXCEPTION_STATES:\n",
    "            retval = self.exception_to_python(retval)\n",
    "        if state in PROPAGATE_STATES:\n",
    "            raise ChordError(f\"Dependency {tid} raised {retval!r}\")\n",
    "        return retval\n",
    "\n",
    "    def set_chord_size(self, group_id, chord_size):\n",
    "        self.set(self.get_key_for_group(group_id, \".s\"), chord_size)\n",
    "\n",
    "    def apply_chord(self, header_result_args, body, **kwargs):\n",
    "        # If any of the child results of this chord are complex (ie. group\n",
    "        # results themselves), we need to save `header_result` to ensure that\n",
    "        # the expected structure is retained when we finish the chord and pass\n",
    "        # the results onward to the body in `on_chord_part_return()`. We don't\n",
    "        # do this is all cases to retain an optimisation in the common case\n",
    "        # where a chord header is comprised of simple result objects.\n",
    "        if not isinstance(header_result_args[1], _regen):\n",
    "            header_result = self.app.GroupResult(*header_result_args)\n",
    "            if any(isinstance(nr, GroupResult) for nr in header_result.results):\n",
    "                header_result.save(backend=self)\n",
    "\n",
    "    @cached_property\n",
    "    def _chord_zset(self):\n",
    "        return self._transport_options.get(\"result_chord_ordered\", True)\n",
    "\n",
    "    @cached_property\n",
    "    def _transport_options(self):\n",
    "        return self.app.conf.get(\"result_backend_transport_options\", {})\n",
    "\n",
    "    def on_chord_part_return(self, request, state, result, propagate=None, **kwargs):\n",
    "        app = self.app\n",
    "        tid, gid, group_index = request.id, request.group, request.group_index\n",
    "        if not gid or not tid:\n",
    "            return\n",
    "        if group_index is None:\n",
    "            group_index = \"+inf\"\n",
    "\n",
    "        client = self.client\n",
    "        jkey = self.get_key_for_group(gid, \".j\")\n",
    "        tkey = self.get_key_for_group(gid, \".t\")\n",
    "        skey = self.get_key_for_group(gid, \".s\")\n",
    "        result = self.encode_result(result, state)\n",
    "        encoded = self.encode([1, tid, state, result])\n",
    "        with client.pipeline() as pipe:\n",
    "            pipeline = (\n",
    "                (\n",
    "                    pipe.zadd(jkey, {encoded: group_index}).zcount(jkey, \"-inf\", \"+inf\")\n",
    "                    if self._chord_zset\n",
    "                    else pipe.rpush(jkey, encoded).llen(jkey)\n",
    "                )\n",
    "                .get(tkey)\n",
    "                .get(skey)\n",
    "            )\n",
    "            if self.expires:\n",
    "                pipeline = (\n",
    "                    pipeline.expire(jkey, self.expires)\n",
    "                    .expire(tkey, self.expires)\n",
    "                    .expire(skey, self.expires)\n",
    "                )\n",
    "\n",
    "            _, readycount, totaldiff, chord_size_bytes = pipeline.execute()[:4]\n",
    "\n",
    "        totaldiff = int(totaldiff or 0)\n",
    "\n",
    "        if chord_size_bytes:\n",
    "            try:\n",
    "                callback = maybe_signature(request.chord, app=app)\n",
    "                total = int(chord_size_bytes) + totaldiff\n",
    "                if readycount == total:\n",
    "                    header_result = GroupResult.restore(gid)\n",
    "                    if header_result is not None:\n",
    "                        # If we manage to restore a `GroupResult`, then it must\n",
    "                        # have been complex and saved by `apply_chord()` earlier.\n",
    "                        #\n",
    "                        # Before we can join the `GroupResult`, it needs to be\n",
    "                        # manually marked as ready to avoid blocking\n",
    "                        header_result.on_ready()\n",
    "                        # We'll `join()` it to get the results and ensure they are\n",
    "                        # structured as intended rather than the flattened version\n",
    "                        # we'd construct without any other information.\n",
    "                        join_func = (\n",
    "                            header_result.join_native\n",
    "                            if header_result.supports_native_join\n",
    "                            else header_result.join\n",
    "                        )\n",
    "                        with allow_join_result():\n",
    "                            resl = join_func(\n",
    "                                timeout=app.conf.result_chord_join_timeout,\n",
    "                                propagate=True,\n",
    "                            )\n",
    "                    else:\n",
    "                        # Otherwise simply extract and decode the results we\n",
    "                        # stashed along the way, which should be faster for large\n",
    "                        # numbers of simple results in the chord header.\n",
    "                        decode, unpack = self.decode, self._unpack_chord_result\n",
    "                        with client.pipeline() as pipe:\n",
    "                            if self._chord_zset:\n",
    "                                pipeline = pipe.zrange(jkey, 0, -1)\n",
    "                            else:\n",
    "                                pipeline = pipe.lrange(jkey, 0, total)\n",
    "                            (resl,) = pipeline.execute()\n",
    "                        resl = [unpack(tup, decode) for tup in resl]\n",
    "                    try:\n",
    "                        callback.delay(resl)\n",
    "                    except Exception as exc:  # pylint: disable=broad-except\n",
    "                        logger.exception(\n",
    "                            \"Chord callback for %r raised: %r\", request.group, exc\n",
    "                        )\n",
    "                        return self.chord_error_from_stack(\n",
    "                            callback,\n",
    "                            ChordError(f\"Callback error: {exc!r}\"),\n",
    "                        )\n",
    "                    finally:\n",
    "                        with client.pipeline() as pipe:\n",
    "                            pipe.delete(jkey).delete(tkey).delete(skey).execute()\n",
    "            except ChordError as exc:\n",
    "                logger.exception(\"Chord %r raised: %r\", request.group, exc)\n",
    "                return self.chord_error_from_stack(callback, exc)\n",
    "            except Exception as exc:  # pylint: disable=broad-except\n",
    "                logger.exception(\"Chord %r raised: %r\", request.group, exc)\n",
    "                return self.chord_error_from_stack(\n",
    "                    callback,\n",
    "                    ChordError(f\"Join error: {exc!r}\"),\n",
    "                )\n",
    "\n",
    "    def _create_client(self, **params):\n",
    "        return self._get_client()(\n",
    "            connection_pool=self._get_pool(**params),\n",
    "        )\n",
    "\n",
    "    def _get_client(self):\n",
    "        return self.redis.StrictRedis\n",
    "\n",
    "    def _get_pool(self, **params):\n",
    "        return self.ConnectionPool(**params)\n",
    "\n",
    "    @property\n",
    "    def ConnectionPool(self):\n",
    "        if self._ConnectionPool is None:\n",
    "            self._ConnectionPool = self.redis.ConnectionPool\n",
    "        return self._ConnectionPool\n",
    "\n",
    "    @cached_property\n",
    "    def client(self):\n",
    "        return self._create_client(**self.connparams)\n",
    "\n",
    "    def __reduce__(self, args=(), kwargs=None):\n",
    "        kwargs = {} if not kwargs else kwargs\n",
    "        return super().__reduce__(\n",
    "            args, dict(kwargs, expires=self.expires, url=self.url)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if getattr(redis, \"sentinel\", None):\n",
    "\n",
    "    class SentinelManagedSSLConnection(\n",
    "        redis.sentinel.SentinelManagedConnection, redis.SSLConnection\n",
    "    ):\n",
    "        \"\"\"Connect to a Redis server using Sentinel + TLS.\n",
    "\n",
    "        Use Sentinel to identify which Redis server is the current master\n",
    "        to connect to and when connecting to the Master server, use an\n",
    "        SSL Connection.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentinelBackend(RedisBackend):\n",
    "    \"\"\"Redis sentinel task result store.\"\"\"\n",
    "\n",
    "    # URL looks like `sentinel://0.0.0.0:26347/3;sentinel://0.0.0.0:26348/3`\n",
    "    _SERVER_URI_SEPARATOR = \";\"\n",
    "\n",
    "    sentinel = getattr(redis, \"sentinel\", None)\n",
    "    connection_class_ssl = SentinelManagedSSLConnection if sentinel else None\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        if self.sentinel is None:\n",
    "            raise ImproperlyConfigured(E_REDIS_SENTINEL_MISSING.strip())\n",
    "\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def as_uri(self, include_password=False):\n",
    "        \"\"\"Return the server addresses as URIs, sanitizing the password or not.\"\"\"\n",
    "        # Allow superclass to do work if we don't need to force sanitization\n",
    "        if include_password:\n",
    "            return super().as_uri(\n",
    "                include_password=include_password,\n",
    "            )\n",
    "        # Otherwise we need to ensure that all components get sanitized rather\n",
    "        # by passing them one by one to the `kombu` helper\n",
    "        uri_chunks = (\n",
    "            maybe_sanitize_url(chunk)\n",
    "            for chunk in (self.url or \"\").split(self._SERVER_URI_SEPARATOR)\n",
    "        )\n",
    "        # Similar to the superclass, strip the trailing slash from URIs with\n",
    "        # all components empty other than the scheme\n",
    "        return self._SERVER_URI_SEPARATOR.join(\n",
    "            uri[:-1] if uri.endswith(\":///\") else uri for uri in uri_chunks\n",
    "        )\n",
    "\n",
    "    def _params_from_url(self, url, defaults):\n",
    "        chunks = url.split(self._SERVER_URI_SEPARATOR)\n",
    "        connparams = dict(defaults, hosts=[])\n",
    "        for chunk in chunks:\n",
    "            data = super()._params_from_url(url=chunk, defaults=defaults)\n",
    "            connparams[\"hosts\"].append(data)\n",
    "        for param in (\"host\", \"port\", \"db\", \"password\"):\n",
    "            connparams.pop(param)\n",
    "\n",
    "        # Adding db/password in connparams to connect to the correct instance\n",
    "        for param in (\"db\", \"password\"):\n",
    "            if connparams[\"hosts\"] and param in connparams[\"hosts\"][0]:\n",
    "                connparams[param] = connparams[\"hosts\"][0].get(param)\n",
    "        return connparams\n",
    "\n",
    "    def _get_sentinel_instance(self, **params):\n",
    "        connparams = params.copy()\n",
    "\n",
    "        hosts = connparams.pop(\"hosts\")\n",
    "        min_other_sentinels = self._transport_options.get(\"min_other_sentinels\", 0)\n",
    "        sentinel_kwargs = self._transport_options.get(\"sentinel_kwargs\", {})\n",
    "\n",
    "        sentinel_instance = self.sentinel.Sentinel(\n",
    "            [(cp[\"host\"], cp[\"port\"]) for cp in hosts],\n",
    "            min_other_sentinels=min_other_sentinels,\n",
    "            sentinel_kwargs=sentinel_kwargs,\n",
    "            **connparams,\n",
    "        )\n",
    "\n",
    "        return sentinel_instance\n",
    "\n",
    "    def _get_pool(self, **params):\n",
    "        sentinel_instance = self._get_sentinel_instance(**params)\n",
    "\n",
    "        master_name = self._transport_options.get(\"master_name\", None)\n",
    "\n",
    "        return sentinel_instance.master_for(\n",
    "            service_name=master_name,\n",
    "            redis_class=self._get_client(),\n",
    "        ).connection_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}