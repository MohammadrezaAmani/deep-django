{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"The ``RPC`` result backend for AMQP brokers.\n",
    "\n",
    "RPC-style result backend, using reply-to and one queue per client.\n",
    "\"\"\"\n",
    "import time\n",
    "\n",
    "import kombu\n",
    "from kombu.common import maybe_declare\n",
    "from kombu.utils.compat import register_after_fork\n",
    "from kombu.utils.objects import cached_property\n",
    "\n",
    "from celery import states\n",
    "from celery._state import current_task, task_join_will_block\n",
    "\n",
    "from . import base\n",
    "from .asynchronous import AsyncBackendMixin, BaseResultConsumer\n",
    "\n",
    "__all__ = (\"BacklogLimitExceeded\", \"RPCBackend\")\n",
    "\n",
    "E_NO_CHORD_SUPPORT = \"\"\"\n",
    "The \"rpc\" result backend does not support chords!\n",
    "\n",
    "Note that a group chained with a task is also upgraded to be a chord,\n",
    "as this pattern requires synchronization.\n",
    "\n",
    "Result backends that supports chords: Redis, Database, Memcached, and more.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BacklogLimitExceeded(Exception):\n",
    "    \"\"\"Too much state history to fast-forward.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _on_after_fork_cleanup_backend(backend):\n",
    "    backend._after_fork()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResultConsumer(BaseResultConsumer):\n",
    "    Consumer = kombu.Consumer\n",
    "\n",
    "    _connection = None\n",
    "    _consumer = None\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._create_binding = self.backend._create_binding\n",
    "\n",
    "    def start(self, initial_task_id, no_ack=True, **kwargs):\n",
    "        self._connection = self.app.connection()\n",
    "        initial_queue = self._create_binding(initial_task_id)\n",
    "        self._consumer = self.Consumer(\n",
    "            self._connection.default_channel,\n",
    "            [initial_queue],\n",
    "            callbacks=[self.on_state_change],\n",
    "            no_ack=no_ack,\n",
    "            accept=self.accept,\n",
    "        )\n",
    "        self._consumer.consume()\n",
    "\n",
    "    def drain_events(self, timeout=None):\n",
    "        if self._connection:\n",
    "            return self._connection.drain_events(timeout=timeout)\n",
    "        elif timeout:\n",
    "            time.sleep(timeout)\n",
    "\n",
    "    def stop(self):\n",
    "        try:\n",
    "            self._consumer.cancel()\n",
    "        finally:\n",
    "            self._connection.close()\n",
    "\n",
    "    def on_after_fork(self):\n",
    "        self._consumer = None\n",
    "        if self._connection is not None:\n",
    "            self._connection.collect()\n",
    "            self._connection = None\n",
    "\n",
    "    def consume_from(self, task_id):\n",
    "        if self._consumer is None:\n",
    "            return self.start(task_id)\n",
    "        queue = self._create_binding(task_id)\n",
    "        if not self._consumer.consuming_from(queue):\n",
    "            self._consumer.add_queue(queue)\n",
    "            self._consumer.consume()\n",
    "\n",
    "    def cancel_for(self, task_id):\n",
    "        if self._consumer:\n",
    "            self._consumer.cancel_by_queue(self._create_binding(task_id).name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPCBackend(base.Backend, AsyncBackendMixin):\n",
    "    \"\"\"Base class for the RPC result backend.\"\"\"\n",
    "\n",
    "    Exchange = kombu.Exchange\n",
    "    Producer = kombu.Producer\n",
    "    ResultConsumer = ResultConsumer\n",
    "\n",
    "    #: Exception raised when there are too many messages for a task id.\n",
    "    BacklogLimitExceeded = BacklogLimitExceeded\n",
    "\n",
    "    persistent = False\n",
    "    supports_autoexpire = True\n",
    "    supports_native_join = True\n",
    "\n",
    "    retry_policy = {\n",
    "        \"max_retries\": 20,\n",
    "        \"interval_start\": 0,\n",
    "        \"interval_step\": 1,\n",
    "        \"interval_max\": 1,\n",
    "    }\n",
    "\n",
    "    class Consumer(kombu.Consumer):\n",
    "        \"\"\"Consumer that requires manual declaration of queues.\"\"\"\n",
    "\n",
    "        auto_declare = False\n",
    "\n",
    "    class Queue(kombu.Queue):\n",
    "        \"\"\"Queue that never caches declaration.\"\"\"\n",
    "\n",
    "        can_cache_declaration = False\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        app,\n",
    "        connection=None,\n",
    "        exchange=None,\n",
    "        exchange_type=None,\n",
    "        persistent=None,\n",
    "        serializer=None,\n",
    "        auto_delete=True,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__(app, **kwargs)\n",
    "        conf = self.app.conf\n",
    "        self._connection = connection\n",
    "        self._out_of_band = {}\n",
    "        self.persistent = self.prepare_persistent(persistent)\n",
    "        self.delivery_mode = 2 if self.persistent else 1\n",
    "        exchange = exchange or conf.result_exchange\n",
    "        exchange_type = exchange_type or conf.result_exchange_type\n",
    "        self.exchange = self._create_exchange(\n",
    "            exchange,\n",
    "            exchange_type,\n",
    "            self.delivery_mode,\n",
    "        )\n",
    "        self.serializer = serializer or conf.result_serializer\n",
    "        self.auto_delete = auto_delete\n",
    "        self.result_consumer = self.ResultConsumer(\n",
    "            self,\n",
    "            self.app,\n",
    "            self.accept,\n",
    "            self._pending_results,\n",
    "            self._pending_messages,\n",
    "        )\n",
    "        if register_after_fork is not None:\n",
    "            register_after_fork(self, _on_after_fork_cleanup_backend)\n",
    "\n",
    "    def _after_fork(self):\n",
    "        # clear state for child processes.\n",
    "        self._pending_results.clear()\n",
    "        self.result_consumer._after_fork()\n",
    "\n",
    "    def _create_exchange(self, name, type=\"direct\", delivery_mode=2):\n",
    "        # uses direct to queue routing (anon exchange).\n",
    "        return self.Exchange(None)\n",
    "\n",
    "    def _create_binding(self, task_id):\n",
    "        \"\"\"Create new binding for task with id.\"\"\"\n",
    "        # RPC backend caches the binding, as one queue is used for all tasks.\n",
    "        return self.binding\n",
    "\n",
    "    def ensure_chords_allowed(self):\n",
    "        raise NotImplementedError(E_NO_CHORD_SUPPORT.strip())\n",
    "\n",
    "    def on_task_call(self, producer, task_id):\n",
    "        # Called every time a task is sent when using this backend.\n",
    "        # We declare the queue we receive replies on in advance of sending\n",
    "        # the message, but we skip this if running in the prefork pool\n",
    "        # (task_join_will_block), as we know the queue is already declared.\n",
    "        if not task_join_will_block():\n",
    "            maybe_declare(self.binding(producer.channel), retry=True)\n",
    "\n",
    "    def destination_for(self, task_id, request):\n",
    "        \"\"\"Get the destination for result by task id.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[str, str]: tuple of ``(reply_to, correlation_id)``.\n",
    "        \"\"\"\n",
    "        # Backends didn't always receive the `request`, so we must still\n",
    "        # support old code that relies on current_task.\n",
    "        try:\n",
    "            request = request or current_task.request\n",
    "        except AttributeError:\n",
    "            raise RuntimeError(f\"RPC backend missing task request for {task_id!r}\")\n",
    "        return request.reply_to, request.correlation_id or task_id\n",
    "\n",
    "    def on_reply_declare(self, task_id):\n",
    "        # Return value here is used as the `declare=` argument\n",
    "        # for Producer.publish.\n",
    "        # By default we don't have to declare anything when sending a result.\n",
    "        pass\n",
    "\n",
    "    def on_result_fulfilled(self, result):\n",
    "        # This usually cancels the queue after the result is received,\n",
    "        # but we don't have to cancel since we have one queue per process.\n",
    "        pass\n",
    "\n",
    "    def as_uri(self, include_password=True):\n",
    "        return \"rpc://\"\n",
    "\n",
    "    def store_result(\n",
    "        self, task_id, result, state, traceback=None, request=None, **kwargs\n",
    "    ):\n",
    "        \"\"\"Send task return value and state.\"\"\"\n",
    "        routing_key, correlation_id = self.destination_for(task_id, request)\n",
    "        if not routing_key:\n",
    "            return\n",
    "        with self.app.amqp.producer_pool.acquire(block=True) as producer:\n",
    "            producer.publish(\n",
    "                self._to_result(task_id, state, result, traceback, request),\n",
    "                exchange=self.exchange,\n",
    "                routing_key=routing_key,\n",
    "                correlation_id=correlation_id,\n",
    "                serializer=self.serializer,\n",
    "                retry=True,\n",
    "                retry_policy=self.retry_policy,\n",
    "                declare=self.on_reply_declare(task_id),\n",
    "                delivery_mode=self.delivery_mode,\n",
    "            )\n",
    "        return result\n",
    "\n",
    "    def _to_result(self, task_id, state, result, traceback, request):\n",
    "        return {\n",
    "            \"task_id\": task_id,\n",
    "            \"status\": state,\n",
    "            \"result\": self.encode_result(result, state),\n",
    "            \"traceback\": traceback,\n",
    "            \"children\": self.current_task_children(request),\n",
    "        }\n",
    "\n",
    "    def on_out_of_band_result(self, task_id, message):\n",
    "        # Callback called when a reply for a task is received,\n",
    "        # but we have no idea what to do with it.\n",
    "        # Since the result is not pending, we put it in a separate\n",
    "        # buffer: probably it will become pending later.\n",
    "        if self.result_consumer:\n",
    "            self.result_consumer.on_out_of_band_result(message)\n",
    "        self._out_of_band[task_id] = message\n",
    "\n",
    "    def get_task_meta(self, task_id, backlog_limit=1000):\n",
    "        buffered = self._out_of_band.pop(task_id, None)\n",
    "        if buffered:\n",
    "            return self._set_cache_by_message(task_id, buffered)\n",
    "\n",
    "        # Polling and using basic_get\n",
    "        latest_by_id = {}\n",
    "        prev = None\n",
    "        for acc in self._slurp_from_queue(task_id, self.accept, backlog_limit):\n",
    "            tid = self._get_message_task_id(acc)\n",
    "            prev, latest_by_id[tid] = latest_by_id.get(tid), acc\n",
    "            if prev:\n",
    "                # backends aren't expected to keep history,\n",
    "                # so we delete everything except the most recent state.\n",
    "                prev.ack()\n",
    "                prev = None\n",
    "\n",
    "        latest = latest_by_id.pop(task_id, None)\n",
    "        for tid, msg in latest_by_id.items():\n",
    "            self.on_out_of_band_result(tid, msg)\n",
    "\n",
    "        if latest:\n",
    "            latest.requeue()\n",
    "            return self._set_cache_by_message(task_id, latest)\n",
    "        else:\n",
    "            # no new state, use previous\n",
    "            try:\n",
    "                return self._cache[task_id]\n",
    "            except KeyError:\n",
    "                # result probably pending.\n",
    "                return {\"status\": states.PENDING, \"result\": None}\n",
    "\n",
    "    poll = get_task_meta  # XXX compat\n",
    "\n",
    "    def _set_cache_by_message(self, task_id, message):\n",
    "        payload = self._cache[task_id] = self.meta_from_decoded(message.payload)\n",
    "        return payload\n",
    "\n",
    "    def _slurp_from_queue(self, task_id, accept, limit=1000, no_ack=False):\n",
    "        with self.app.pool.acquire_channel(block=True) as (_, channel):\n",
    "            binding = self._create_binding(task_id)(channel)\n",
    "            binding.declare()\n",
    "\n",
    "            for _ in range(limit):\n",
    "                msg = binding.get(accept=accept, no_ack=no_ack)\n",
    "                if not msg:\n",
    "                    break\n",
    "                yield msg\n",
    "            else:\n",
    "                raise self.BacklogLimitExceeded(task_id)\n",
    "\n",
    "    def _get_message_task_id(self, message):\n",
    "        try:\n",
    "            # try property first so we don't have to deserialize\n",
    "            # the payload.\n",
    "            return message.properties[\"correlation_id\"]\n",
    "        except (AttributeError, KeyError):\n",
    "            # message sent by old Celery version, need to deserialize.\n",
    "            return message.payload[\"task_id\"]\n",
    "\n",
    "    def revive(self, channel):\n",
    "        pass\n",
    "\n",
    "    def reload_task_result(self, task_id):\n",
    "        raise NotImplementedError(\n",
    "            \"reload_task_result is not supported by this backend.\"\n",
    "        )\n",
    "\n",
    "    def reload_group_result(self, task_id):\n",
    "        \"\"\"Reload group result, even if it has been previously fetched.\"\"\"\n",
    "        raise NotImplementedError(\n",
    "            \"reload_group_result is not supported by this backend.\"\n",
    "        )\n",
    "\n",
    "    def save_group(self, group_id, result):\n",
    "        raise NotImplementedError(\"save_group is not supported by this backend.\")\n",
    "\n",
    "    def restore_group(self, group_id, cache=True):\n",
    "        raise NotImplementedError(\"restore_group is not supported by this backend.\")\n",
    "\n",
    "    def delete_group(self, group_id):\n",
    "        raise NotImplementedError(\"delete_group is not supported by this backend.\")\n",
    "\n",
    "    def __reduce__(self, args=(), kwargs=None):\n",
    "        kwargs = {} if not kwargs else kwargs\n",
    "        return super().__reduce__(\n",
    "            args,\n",
    "            dict(\n",
    "                kwargs,\n",
    "                connection=self._connection,\n",
    "                exchange=self.exchange.name,\n",
    "                exchange_type=self.exchange.type,\n",
    "                persistent=self.persistent,\n",
    "                serializer=self.serializer,\n",
    "                auto_delete=self.auto_delete,\n",
    "                expires=self.expires,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def binding(self):\n",
    "        return self.Queue(\n",
    "            self.oid,\n",
    "            self.exchange,\n",
    "            self.oid,\n",
    "            durable=False,\n",
    "            auto_delete=True,\n",
    "            expires=self.expires,\n",
    "        )\n",
    "\n",
    "    @cached_property\n",
    "    def oid(self):\n",
    "        # cached here is the app thread OID: name of queue we receive results on.\n",
    "        return self.app.thread_oid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}