{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Google Cloud Storage result store backend for Celery.\"\"\"\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import datetime, timedelta\n",
    "from os import getpid\n",
    "from threading import RLock\n",
    "\n",
    "from kombu.utils.encoding import bytes_to_str\n",
    "from kombu.utils.functional import dictfilter\n",
    "from kombu.utils.url import url_to_parts\n",
    "\n",
    "from celery.exceptions import ImproperlyConfigured\n",
    "\n",
    "from .base import KeyValueStoreBackend\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "    from google.cloud import storage\n",
    "    from google.cloud.storage import Client\n",
    "    from google.cloud.storage.retry import DEFAULT_RETRY\n",
    "except ImportError:\n",
    "    storage = None\n",
    "\n",
    "__all__ = (\"GCSBackend\",)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCSBackend(KeyValueStoreBackend):\n",
    "    \"\"\"Google Cloud Storage task result backend.\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._lock = RLock()\n",
    "        self._pid = getpid()\n",
    "        self._retry_policy = DEFAULT_RETRY\n",
    "        self._client = None\n",
    "\n",
    "        if not storage:\n",
    "            raise ImproperlyConfigured(\n",
    "                \"You must install google-cloud-storage to use gcs backend\"\n",
    "            )\n",
    "        conf = self.app.conf\n",
    "        if self.url:\n",
    "            url_params = self._params_from_url()\n",
    "            conf.update(**dictfilter(url_params))\n",
    "\n",
    "        self.bucket_name = conf.get(\"gcs_bucket\")\n",
    "        if not self.bucket_name:\n",
    "            raise ImproperlyConfigured(\n",
    "                \"Missing bucket name: specify gcs_bucket to use gcs backend\"\n",
    "            )\n",
    "        self.project = conf.get(\"gcs_project\")\n",
    "        if not self.project:\n",
    "            raise ImproperlyConfigured(\n",
    "                \"Missing project:specify gcs_project to use gcs backend\"\n",
    "            )\n",
    "        self.base_path = conf.get(\"gcs_base_path\", \"\").strip(\"/\")\n",
    "        self._threadpool_maxsize = int(conf.get(\"gcs_threadpool_maxsize\", 10))\n",
    "        self.ttl = float(conf.get(\"gcs_ttl\") or 0)\n",
    "        if self.ttl < 0:\n",
    "            raise ImproperlyConfigured(\n",
    "                f\"Invalid ttl: {self.ttl} must be greater than or equal to 0\"\n",
    "            )\n",
    "        elif self.ttl:\n",
    "            if not self._is_bucket_lifecycle_rule_exists():\n",
    "                raise ImproperlyConfigured(\n",
    "                    f\"Missing lifecycle rule to use gcs backend with ttl on \"\n",
    "                    f\"bucket: {self.bucket_name}\"\n",
    "                )\n",
    "\n",
    "    def get(self, key):\n",
    "        key = bytes_to_str(key)\n",
    "        blob = self._get_blob(key)\n",
    "        try:\n",
    "            return blob.download_as_bytes(retry=self._retry_policy)\n",
    "        except storage.blob.NotFound:\n",
    "            return None\n",
    "\n",
    "    def set(self, key, value):\n",
    "        key = bytes_to_str(key)\n",
    "        blob = self._get_blob(key)\n",
    "        if self.ttl:\n",
    "            blob.custom_time = datetime.utcnow() + timedelta(seconds=self.ttl)\n",
    "        blob.upload_from_string(value, retry=self._retry_policy)\n",
    "\n",
    "    def delete(self, key):\n",
    "        key = bytes_to_str(key)\n",
    "        blob = self._get_blob(key)\n",
    "        if blob.exists():\n",
    "            blob.delete(retry=self._retry_policy)\n",
    "\n",
    "    def mget(self, keys):\n",
    "        with ThreadPoolExecutor() as pool:\n",
    "            return list(pool.map(self.get, keys))\n",
    "\n",
    "    @property\n",
    "    def client(self):\n",
    "        \"\"\"Returns a storage client.\"\"\"\n",
    "\n",
    "        # make sure it's thread-safe, as creating a new client is expensive\n",
    "        with self._lock:\n",
    "            if self._client and self._pid == getpid():\n",
    "                return self._client\n",
    "            # make sure each process gets its own connection after a fork\n",
    "            self._client = Client(project=self.project)\n",
    "            self._pid = getpid()\n",
    "\n",
    "            # config the number of connections to the server\n",
    "            adapter = requests.adapters.HTTPAdapter(\n",
    "                pool_connections=self._threadpool_maxsize,\n",
    "                pool_maxsize=self._threadpool_maxsize,\n",
    "                max_retries=3,\n",
    "            )\n",
    "            client_http = self._client._http\n",
    "            client_http.mount(\"https://\", adapter)\n",
    "            client_http._auth_request.session.mount(\"https://\", adapter)\n",
    "\n",
    "            return self._client\n",
    "\n",
    "    @property\n",
    "    def bucket(self):\n",
    "        return self.client.bucket(self.bucket_name)\n",
    "\n",
    "    def _get_blob(self, key):\n",
    "        key_bucket_path = f\"{self.base_path}/{key}\" if self.base_path else key\n",
    "        return self.bucket.blob(key_bucket_path)\n",
    "\n",
    "    def _is_bucket_lifecycle_rule_exists(self):\n",
    "        bucket = self.bucket\n",
    "        bucket.reload()\n",
    "        for rule in bucket.lifecycle_rules:\n",
    "            if rule[\"action\"][\"type\"] == \"Delete\":\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _params_from_url(self):\n",
    "        url_parts = url_to_parts(self.url)\n",
    "\n",
    "        return {\n",
    "            \"gcs_bucket\": url_parts.hostname,\n",
    "            \"gcs_base_path\": url_parts.path,\n",
    "            **url_parts.query,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}