{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import logging\n",
    "import os\n",
    "import signal\n",
    "import subprocess\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "import traceback\n",
    "import weakref\n",
    "from collections import defaultdict\n",
    "from functools import lru_cache, wraps\n",
    "from pathlib import Path\n",
    "from types import ModuleType\n",
    "from zipimport import zipimporter\n",
    "\n",
    "import django\n",
    "from django.apps import apps\n",
    "from django.core.signals import request_finished\n",
    "from django.dispatch import Signal\n",
    "from django.utils.functional import cached_property\n",
    "from django.utils.version import get_version_tuple\n",
    "\n",
    "autoreload_started = Signal()\n",
    "file_changed = Signal()\n",
    "\n",
    "DJANGO_AUTORELOAD_ENV = \"RUN_MAIN\"\n",
    "\n",
    "logger = logging.getLogger(\"django.utils.autoreload\")\n",
    "\n",
    "# If an error is raised while importing a file, it's not placed in sys.modules.\n",
    "# This means that any future modifications aren't caught. Keep a list of these\n",
    "# file paths to allow watching them in the future.\n",
    "_error_files = []\n",
    "_exception = None\n",
    "\n",
    "try:\n",
    "    import termios\n",
    "except ImportError:\n",
    "    termios = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import pywatchman\n",
    "except ImportError:\n",
    "    pywatchman = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_django_module(module):\n",
    "    \"\"\"Return True if the given module is nested under Django.\"\"\"\n",
    "    return module.__name__.startswith(\"django.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_django_path(path):\n",
    "    \"\"\"Return True if the given file path is nested under Django.\"\"\"\n",
    "    return Path(django.__file__).parent in Path(path).parents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_errors(fn):\n",
    "    @wraps(fn)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        global _exception\n",
    "        try:\n",
    "            fn(*args, **kwargs)\n",
    "        except Exception:\n",
    "            _exception = sys.exc_info()\n",
    "\n",
    "            et, ev, tb = _exception\n",
    "\n",
    "            if getattr(ev, \"filename\", None) is None:\n",
    "                # get the filename from the last item in the stack\n",
    "                filename = traceback.extract_tb(tb)[-1][0]\n",
    "            else:\n",
    "                filename = ev.filename\n",
    "\n",
    "            if filename not in _error_files:\n",
    "                _error_files.append(filename)\n",
    "\n",
    "            raise\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raise_last_exception():\n",
    "    global _exception\n",
    "    if _exception is not None:\n",
    "        raise _exception[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_echo_on():\n",
    "    \"\"\"\n",
    "    Ensure that echo mode is enabled. Some tools such as PDB disable\n",
    "    it which causes usability issues after reload.\n",
    "    \"\"\"\n",
    "    if not termios or not sys.stdin.isatty():\n",
    "        return\n",
    "    attr_list = termios.tcgetattr(sys.stdin)\n",
    "    if not attr_list[3] & termios.ECHO:\n",
    "        attr_list[3] |= termios.ECHO\n",
    "        if hasattr(signal, \"SIGTTOU\"):\n",
    "            old_handler = signal.signal(signal.SIGTTOU, signal.SIG_IGN)\n",
    "        else:\n",
    "            old_handler = None\n",
    "        termios.tcsetattr(sys.stdin, termios.TCSANOW, attr_list)\n",
    "        if old_handler is not None:\n",
    "            signal.signal(signal.SIGTTOU, old_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iter_all_python_module_files():\n",
    "    # This is a hot path during reloading. Create a stable sorted list of\n",
    "    # modules based on the module name and pass it to iter_modules_and_files().\n",
    "    # This ensures cached results are returned in the usual case that modules\n",
    "    # aren't loaded on the fly.\n",
    "    keys = sorted(sys.modules)\n",
    "    modules = tuple(\n",
    "        m\n",
    "        for m in map(sys.modules.__getitem__, keys)\n",
    "        if not isinstance(m, weakref.ProxyTypes)\n",
    "    )\n",
    "    return iter_modules_and_files(modules, frozenset(_error_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=1)\n",
    "def iter_modules_and_files(modules, extra_files):\n",
    "    \"\"\"Iterate through all modules needed to be watched.\"\"\"\n",
    "    sys_file_paths = []\n",
    "    for module in modules:\n",
    "        # During debugging (with PyDev) the 'typing.io' and 'typing.re' objects\n",
    "        # are added to sys.modules, however they are types not modules and so\n",
    "        # cause issues here.\n",
    "        if not isinstance(module, ModuleType):\n",
    "            continue\n",
    "        if module.__name__ in (\"__main__\", \"__mp_main__\"):\n",
    "            # __main__ (usually manage.py) doesn't always have a __spec__ set.\n",
    "            # Handle this by falling back to using __file__, resolved below.\n",
    "            # See https://docs.python.org/reference/import.html#main-spec\n",
    "            # __file__ may not exists, e.g. when running ipdb debugger.\n",
    "            if hasattr(module, \"__file__\"):\n",
    "                sys_file_paths.append(module.__file__)\n",
    "            continue\n",
    "        if getattr(module, \"__spec__\", None) is None:\n",
    "            continue\n",
    "        spec = module.__spec__\n",
    "        # Modules could be loaded from places without a concrete location. If\n",
    "        # this is the case, skip them.\n",
    "        if spec.has_location:\n",
    "            origin = (\n",
    "                spec.loader.archive\n",
    "                if isinstance(spec.loader, zipimporter)\n",
    "                else spec.origin\n",
    "            )\n",
    "            sys_file_paths.append(origin)\n",
    "\n",
    "    results = set()\n",
    "    for filename in itertools.chain(sys_file_paths, extra_files):\n",
    "        if not filename:\n",
    "            continue\n",
    "        path = Path(filename)\n",
    "        try:\n",
    "            if not path.exists():\n",
    "                # The module could have been removed, don't fail loudly if this\n",
    "                # is the case.\n",
    "                continue\n",
    "        except ValueError as e:\n",
    "            # Network filesystems may return null bytes in file paths.\n",
    "            logger.debug('\"%s\" raised when resolving path: \"%s\"', e, path)\n",
    "            continue\n",
    "        resolved_path = path.resolve().absolute()\n",
    "        results.add(resolved_path)\n",
    "    return frozenset(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=1)\n",
    "def common_roots(paths):\n",
    "    \"\"\"\n",
    "    Return a tuple of common roots that are shared between the given paths.\n",
    "    File system watchers operate on directories and aren't cheap to create.\n",
    "    Try to find the minimum set of directories to watch that encompass all of\n",
    "    the files that need to be watched.\n",
    "    \"\"\"\n",
    "    # Inspired from Werkzeug:\n",
    "    # https://github.com/pallets/werkzeug/blob/7477be2853df70a022d9613e765581b9411c3c39/werkzeug/_reloader.py\n",
    "    # Create a sorted list of the path components, longest first.\n",
    "    path_parts = sorted([x.parts for x in paths], key=len, reverse=True)\n",
    "    tree = {}\n",
    "    for chunks in path_parts:\n",
    "        node = tree\n",
    "        # Add each part of the path to the tree.\n",
    "        for chunk in chunks:\n",
    "            node = node.setdefault(chunk, {})\n",
    "        # Clear the last leaf in the tree.\n",
    "        node.clear()\n",
    "\n",
    "    # Turn the tree into a list of Path instances.\n",
    "    def _walk(node, path):\n",
    "        for prefix, child in node.items():\n",
    "            yield from _walk(child, path + (prefix,))\n",
    "        if not node:\n",
    "            yield Path(*path)\n",
    "\n",
    "    return tuple(_walk(tree, ()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sys_path_directories():\n",
    "    \"\"\"\n",
    "    Yield absolute directories from sys.path, ignoring entries that don't\n",
    "    exist.\n",
    "    \"\"\"\n",
    "    for path in sys.path:\n",
    "        path = Path(path)\n",
    "        if not path.exists():\n",
    "            continue\n",
    "        resolved_path = path.resolve().absolute()\n",
    "        # If the path is a file (like a zip file), watch the parent directory.\n",
    "        if resolved_path.is_file():\n",
    "            yield resolved_path.parent\n",
    "        else:\n",
    "            yield resolved_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_child_arguments():\n",
    "    \"\"\"\n",
    "    Return the executable. This contains a workaround for Windows if the\n",
    "    executable is reported to not have the .exe extension which can cause bugs\n",
    "    on reloading.\n",
    "    \"\"\"\n",
    "    import __main__\n",
    "\n",
    "    py_script = Path(sys.argv[0])\n",
    "    exe_entrypoint = py_script.with_suffix(\".exe\")\n",
    "\n",
    "    args = [sys.executable] + [\"-W%s\" % o for o in sys.warnoptions]\n",
    "    if sys.implementation.name == \"cpython\":\n",
    "        args.extend(\n",
    "            f\"-X{key}\" if value is True else f\"-X{key}={value}\"\n",
    "            for key, value in sys._xoptions.items()\n",
    "        )\n",
    "    # __spec__ is set when the server was started with the `-m` option,\n",
    "    # see https://docs.python.org/3/reference/import.html#main-spec\n",
    "    # __spec__ may not exist, e.g. when running in a Conda env.\n",
    "    if getattr(__main__, \"__spec__\", None) is not None and not exe_entrypoint.exists():\n",
    "        spec = __main__.__spec__\n",
    "        if (spec.name == \"__main__\" or spec.name.endswith(\".__main__\")) and spec.parent:\n",
    "            name = spec.parent\n",
    "        else:\n",
    "            name = spec.name\n",
    "        args += [\"-m\", name]\n",
    "        args += sys.argv[1:]\n",
    "    elif not py_script.exists():\n",
    "        # sys.argv[0] may not exist for several reasons on Windows.\n",
    "        # It may exist with a .exe extension or have a -script.py suffix.\n",
    "        if exe_entrypoint.exists():\n",
    "            # Should be executed directly, ignoring sys.executable.\n",
    "            return [exe_entrypoint, *sys.argv[1:]]\n",
    "        script_entrypoint = py_script.with_name(\"%s-script.py\" % py_script.name)\n",
    "        if script_entrypoint.exists():\n",
    "            # Should be executed as usual.\n",
    "            return [*args, script_entrypoint, *sys.argv[1:]]\n",
    "        raise RuntimeError(\"Script %s does not exist.\" % py_script)\n",
    "    else:\n",
    "        args += sys.argv\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigger_reload(filename):\n",
    "    logger.info(\"%s changed, reloading.\", filename)\n",
    "    sys.exit(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restart_with_reloader():\n",
    "    new_environ = {**os.environ, DJANGO_AUTORELOAD_ENV: \"true\"}\n",
    "    args = get_child_arguments()\n",
    "    while True:\n",
    "        p = subprocess.run(args, env=new_environ, close_fds=False)\n",
    "        if p.returncode != 3:\n",
    "            return p.returncode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseReloader:\n",
    "    def __init__(self):\n",
    "        self.extra_files = set()\n",
    "        self.directory_globs = defaultdict(set)\n",
    "        self._stop_condition = threading.Event()\n",
    "\n",
    "    def watch_dir(self, path, glob):\n",
    "        path = Path(path)\n",
    "        try:\n",
    "            path = path.absolute()\n",
    "        except FileNotFoundError:\n",
    "            logger.debug(\n",
    "                \"Unable to watch directory %s as it cannot be resolved.\",\n",
    "                path,\n",
    "                exc_info=True,\n",
    "            )\n",
    "            return\n",
    "        logger.debug(\"Watching dir %s with glob %s.\", path, glob)\n",
    "        self.directory_globs[path].add(glob)\n",
    "\n",
    "    def watched_files(self, include_globs=True):\n",
    "        \"\"\"\n",
    "        Yield all files that need to be watched, including module files and\n",
    "        files within globs.\n",
    "        \"\"\"\n",
    "        yield from iter_all_python_module_files()\n",
    "        yield from self.extra_files\n",
    "        if include_globs:\n",
    "            for directory, patterns in self.directory_globs.items():\n",
    "                for pattern in patterns:\n",
    "                    yield from directory.glob(pattern)\n",
    "\n",
    "    def wait_for_apps_ready(self, app_reg, django_main_thread):\n",
    "        \"\"\"\n",
    "        Wait until Django reports that the apps have been loaded. If the given\n",
    "        thread has terminated before the apps are ready, then a SyntaxError or\n",
    "        other non-recoverable error has been raised. In that case, stop waiting\n",
    "        for the apps_ready event and continue processing.\n",
    "\n",
    "        Return True if the thread is alive and the ready event has been\n",
    "        triggered, or False if the thread is terminated while waiting for the\n",
    "        event.\n",
    "        \"\"\"\n",
    "        while django_main_thread.is_alive():\n",
    "            if app_reg.ready_event.wait(timeout=0.1):\n",
    "                return True\n",
    "        else:\n",
    "            logger.debug(\"Main Django thread has terminated before apps are ready.\")\n",
    "            return False\n",
    "\n",
    "    def run(self, django_main_thread):\n",
    "        logger.debug(\"Waiting for apps ready_event.\")\n",
    "        self.wait_for_apps_ready(apps, django_main_thread)\n",
    "        from django.urls import get_resolver\n",
    "\n",
    "        # Prevent a race condition where URL modules aren't loaded when the\n",
    "        # reloader starts by accessing the urlconf_module property.\n",
    "        try:\n",
    "            get_resolver().urlconf_module\n",
    "        except Exception:\n",
    "            # Loading the urlconf can result in errors during development.\n",
    "            # If this occurs then swallow the error and continue.\n",
    "            pass\n",
    "        logger.debug(\"Apps ready_event triggered. Sending autoreload_started signal.\")\n",
    "        autoreload_started.send(sender=self)\n",
    "        self.run_loop()\n",
    "\n",
    "    def run_loop(self):\n",
    "        ticker = self.tick()\n",
    "        while not self.should_stop:\n",
    "            try:\n",
    "                next(ticker)\n",
    "            except StopIteration:\n",
    "                break\n",
    "        self.stop()\n",
    "\n",
    "    def tick(self):\n",
    "        \"\"\"\n",
    "        This generator is called in a loop from run_loop. It's important that\n",
    "        the method takes care of pausing or otherwise waiting for a period of\n",
    "        time. This split between run_loop() and tick() is to improve the\n",
    "        testability of the reloader implementations by decoupling the work they\n",
    "        do from the loop.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"subclasses must implement tick().\")\n",
    "\n",
    "    @classmethod\n",
    "    def check_availability(cls):\n",
    "        raise NotImplementedError(\"subclasses must implement check_availability().\")\n",
    "\n",
    "    def notify_file_changed(self, path):\n",
    "        results = file_changed.send(sender=self, file_path=path)\n",
    "        logger.debug(\"%s notified as changed. Signal results: %s.\", path, results)\n",
    "        if not any(res[1] for res in results):\n",
    "            trigger_reload(path)\n",
    "\n",
    "    # These are primarily used for testing.\n",
    "    @property\n",
    "    def should_stop(self):\n",
    "        return self._stop_condition.is_set()\n",
    "\n",
    "    def stop(self):\n",
    "        self._stop_condition.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatReloader(BaseReloader):\n",
    "    SLEEP_TIME = 1  # Check for changes once per second.\n",
    "\n",
    "    def tick(self):\n",
    "        mtimes = {}\n",
    "        while True:\n",
    "            for filepath, mtime in self.snapshot_files():\n",
    "                old_time = mtimes.get(filepath)\n",
    "                mtimes[filepath] = mtime\n",
    "                if old_time is None:\n",
    "                    logger.debug(\"File %s first seen with mtime %s\", filepath, mtime)\n",
    "                    continue\n",
    "                elif mtime > old_time:\n",
    "                    logger.debug(\n",
    "                        \"File %s previous mtime: %s, current mtime: %s\",\n",
    "                        filepath,\n",
    "                        old_time,\n",
    "                        mtime,\n",
    "                    )\n",
    "                    self.notify_file_changed(filepath)\n",
    "\n",
    "            time.sleep(self.SLEEP_TIME)\n",
    "            yield\n",
    "\n",
    "    def snapshot_files(self):\n",
    "        # watched_files may produce duplicate paths if globs overlap.\n",
    "        seen_files = set()\n",
    "        for file in self.watched_files():\n",
    "            if file in seen_files:\n",
    "                continue\n",
    "            try:\n",
    "                mtime = file.stat().st_mtime\n",
    "            except OSError:\n",
    "                # This is thrown when the file does not exist.\n",
    "                continue\n",
    "            seen_files.add(file)\n",
    "            yield file, mtime\n",
    "\n",
    "    @classmethod\n",
    "    def check_availability(cls):\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WatchmanUnavailable(RuntimeError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WatchmanReloader(BaseReloader):\n",
    "    def __init__(self):\n",
    "        self.roots = defaultdict(set)\n",
    "        self.processed_request = threading.Event()\n",
    "        self.client_timeout = int(os.environ.get(\"DJANGO_WATCHMAN_TIMEOUT\", 5))\n",
    "        super().__init__()\n",
    "\n",
    "    @cached_property\n",
    "    def client(self):\n",
    "        return pywatchman.client(timeout=self.client_timeout)\n",
    "\n",
    "    def _watch_root(self, root):\n",
    "        # In practice this shouldn't occur, however, it's possible that a\n",
    "        # directory that doesn't exist yet is being watched. If it's outside of\n",
    "        # sys.path then this will end up a new root. How to handle this isn't\n",
    "        # clear: Not adding the root will likely break when subscribing to the\n",
    "        # changes, however, as this is currently an internal API,  no files\n",
    "        # will be being watched outside of sys.path. Fixing this by checking\n",
    "        # inside watch_glob() and watch_dir() is expensive, instead this could\n",
    "        # could fall back to the StatReloader if this case is detected? For\n",
    "        # now, watching its parent, if possible, is sufficient.\n",
    "        if not root.exists():\n",
    "            if not root.parent.exists():\n",
    "                logger.warning(\n",
    "                    \"Unable to watch root dir %s as neither it or its parent exist.\",\n",
    "                    root,\n",
    "                )\n",
    "                return\n",
    "            root = root.parent\n",
    "        result = self.client.query(\"watch-project\", str(root.absolute()))\n",
    "        if \"warning\" in result:\n",
    "            logger.warning(\"Watchman warning: %s\", result[\"warning\"])\n",
    "        logger.debug(\"Watchman watch-project result: %s\", result)\n",
    "        return result[\"watch\"], result.get(\"relative_path\")\n",
    "\n",
    "    @lru_cache\n",
    "    def _get_clock(self, root):\n",
    "        return self.client.query(\"clock\", root)[\"clock\"]\n",
    "\n",
    "    def _subscribe(self, directory, name, expression):\n",
    "        root, rel_path = self._watch_root(directory)\n",
    "        # Only receive notifications of files changing, filtering out other types\n",
    "        # like special files: https://facebook.github.io/watchman/docs/type\n",
    "        only_files_expression = [\n",
    "            \"allof\",\n",
    "            [\"anyof\", [\"type\", \"f\"], [\"type\", \"l\"]],\n",
    "            expression,\n",
    "        ]\n",
    "        query = {\n",
    "            \"expression\": only_files_expression,\n",
    "            \"fields\": [\"name\"],\n",
    "            \"since\": self._get_clock(root),\n",
    "            \"dedup_results\": True,\n",
    "        }\n",
    "        if rel_path:\n",
    "            query[\"relative_root\"] = rel_path\n",
    "        logger.debug(\n",
    "            \"Issuing watchman subscription %s, for root %s. Query: %s\",\n",
    "            name,\n",
    "            root,\n",
    "            query,\n",
    "        )\n",
    "        self.client.query(\"subscribe\", root, name, query)\n",
    "\n",
    "    def _subscribe_dir(self, directory, filenames):\n",
    "        if not directory.exists():\n",
    "            if not directory.parent.exists():\n",
    "                logger.warning(\n",
    "                    \"Unable to watch directory %s as neither it or its parent exist.\",\n",
    "                    directory,\n",
    "                )\n",
    "                return\n",
    "            prefix = \"files-parent-%s\" % directory.name\n",
    "            filenames = [\"%s/%s\" % (directory.name, filename) for filename in filenames]\n",
    "            directory = directory.parent\n",
    "            expression = [\"name\", filenames, \"wholename\"]\n",
    "        else:\n",
    "            prefix = \"files\"\n",
    "            expression = [\"name\", filenames]\n",
    "        self._subscribe(directory, \"%s:%s\" % (prefix, directory), expression)\n",
    "\n",
    "    def _watch_glob(self, directory, patterns):\n",
    "        \"\"\"\n",
    "        Watch a directory with a specific glob. If the directory doesn't yet\n",
    "        exist, attempt to watch the parent directory and amend the patterns to\n",
    "        include this. It's important this method isn't called more than one per\n",
    "        directory when updating all subscriptions. Subsequent calls will\n",
    "        overwrite the named subscription, so it must include all possible glob\n",
    "        expressions.\n",
    "        \"\"\"\n",
    "        prefix = \"glob\"\n",
    "        if not directory.exists():\n",
    "            if not directory.parent.exists():\n",
    "                logger.warning(\n",
    "                    \"Unable to watch directory %s as neither it or its parent exist.\",\n",
    "                    directory,\n",
    "                )\n",
    "                return\n",
    "            prefix = \"glob-parent-%s\" % directory.name\n",
    "            patterns = [\"%s/%s\" % (directory.name, pattern) for pattern in patterns]\n",
    "            directory = directory.parent\n",
    "\n",
    "        expression = [\"anyof\"]\n",
    "        for pattern in patterns:\n",
    "            expression.append([\"match\", pattern, \"wholename\"])\n",
    "        self._subscribe(directory, \"%s:%s\" % (prefix, directory), expression)\n",
    "\n",
    "    def watched_roots(self, watched_files):\n",
    "        extra_directories = self.directory_globs.keys()\n",
    "        watched_file_dirs = [f.parent for f in watched_files]\n",
    "        sys_paths = list(sys_path_directories())\n",
    "        return frozenset((*extra_directories, *watched_file_dirs, *sys_paths))\n",
    "\n",
    "    def _update_watches(self):\n",
    "        watched_files = list(self.watched_files(include_globs=False))\n",
    "        found_roots = common_roots(self.watched_roots(watched_files))\n",
    "        logger.debug(\"Watching %s files\", len(watched_files))\n",
    "        logger.debug(\"Found common roots: %s\", found_roots)\n",
    "        # Setup initial roots for performance, shortest roots first.\n",
    "        for root in sorted(found_roots):\n",
    "            self._watch_root(root)\n",
    "        for directory, patterns in self.directory_globs.items():\n",
    "            self._watch_glob(directory, patterns)\n",
    "        # Group sorted watched_files by their parent directory.\n",
    "        sorted_files = sorted(watched_files, key=lambda p: p.parent)\n",
    "        for directory, group in itertools.groupby(sorted_files, key=lambda p: p.parent):\n",
    "            # These paths need to be relative to the parent directory.\n",
    "            self._subscribe_dir(\n",
    "                directory, [str(p.relative_to(directory)) for p in group]\n",
    "            )\n",
    "\n",
    "    def update_watches(self):\n",
    "        try:\n",
    "            self._update_watches()\n",
    "        except Exception as ex:\n",
    "            # If the service is still available, raise the original exception.\n",
    "            if self.check_server_status(ex):\n",
    "                raise\n",
    "\n",
    "    def _check_subscription(self, sub):\n",
    "        subscription = self.client.getSubscription(sub)\n",
    "        if not subscription:\n",
    "            return\n",
    "        logger.debug(\"Watchman subscription %s has results.\", sub)\n",
    "        for result in subscription:\n",
    "            # When using watch-project, it's not simple to get the relative\n",
    "            # directory without storing some specific state. Store the full\n",
    "            # path to the directory in the subscription name, prefixed by its\n",
    "            # type (glob, files).\n",
    "            root_directory = Path(result[\"subscription\"].split(\":\", 1)[1])\n",
    "            logger.debug(\"Found root directory %s\", root_directory)\n",
    "            for file in result.get(\"files\", []):\n",
    "                self.notify_file_changed(root_directory / file)\n",
    "\n",
    "    def request_processed(self, **kwargs):\n",
    "        logger.debug(\"Request processed. Setting update_watches event.\")\n",
    "        self.processed_request.set()\n",
    "\n",
    "    def tick(self):\n",
    "        request_finished.connect(self.request_processed)\n",
    "        self.update_watches()\n",
    "        while True:\n",
    "            if self.processed_request.is_set():\n",
    "                self.update_watches()\n",
    "                self.processed_request.clear()\n",
    "            try:\n",
    "                self.client.receive()\n",
    "            except pywatchman.SocketTimeout:\n",
    "                pass\n",
    "            except pywatchman.WatchmanError as ex:\n",
    "                logger.debug(\"Watchman error: %s, checking server status.\", ex)\n",
    "                self.check_server_status(ex)\n",
    "            else:\n",
    "                for sub in list(self.client.subs.keys()):\n",
    "                    self._check_subscription(sub)\n",
    "            yield\n",
    "            # Protect against busy loops.\n",
    "            time.sleep(0.1)\n",
    "\n",
    "    def stop(self):\n",
    "        self.client.close()\n",
    "        super().stop()\n",
    "\n",
    "    def check_server_status(self, inner_ex=None):\n",
    "        \"\"\"Return True if the server is available.\"\"\"\n",
    "        try:\n",
    "            self.client.query(\"version\")\n",
    "        except Exception:\n",
    "            raise WatchmanUnavailable(str(inner_ex)) from inner_ex\n",
    "        return True\n",
    "\n",
    "    @classmethod\n",
    "    def check_availability(cls):\n",
    "        if not pywatchman:\n",
    "            raise WatchmanUnavailable(\"pywatchman not installed.\")\n",
    "        client = pywatchman.client(timeout=0.1)\n",
    "        try:\n",
    "            result = client.capabilityCheck()\n",
    "        except Exception:\n",
    "            # The service is down?\n",
    "            raise WatchmanUnavailable(\"Cannot connect to the watchman service.\")\n",
    "        version = get_version_tuple(result[\"version\"])\n",
    "        # Watchman 4.9 includes multiple improvements to watching project\n",
    "        # directories as well as case insensitive filesystems.\n",
    "        logger.debug(\"Watchman version %s\", version)\n",
    "        if version < (4, 9):\n",
    "            raise WatchmanUnavailable(\"Watchman 4.9 or later is required.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reloader():\n",
    "    \"\"\"Return the most suitable reloader for this environment.\"\"\"\n",
    "    try:\n",
    "        WatchmanReloader.check_availability()\n",
    "    except WatchmanUnavailable:\n",
    "        return StatReloader()\n",
    "    return WatchmanReloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_django(reloader, main_func, *args, **kwargs):\n",
    "    ensure_echo_on()\n",
    "\n",
    "    main_func = check_errors(main_func)\n",
    "    django_main_thread = threading.Thread(\n",
    "        target=main_func, args=args, kwargs=kwargs, name=\"django-main-thread\"\n",
    "    )\n",
    "    django_main_thread.daemon = True\n",
    "    django_main_thread.start()\n",
    "\n",
    "    while not reloader.should_stop:\n",
    "        reloader.run(django_main_thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_with_reloader(main_func, *args, **kwargs):\n",
    "    signal.signal(signal.SIGTERM, lambda *args: sys.exit(0))\n",
    "    try:\n",
    "        if os.environ.get(DJANGO_AUTORELOAD_ENV) == \"true\":\n",
    "            reloader = get_reloader()\n",
    "            logger.info(\n",
    "                \"Watching for file changes with %s\", reloader.__class__.__name__\n",
    "            )\n",
    "            start_django(reloader, main_func, *args, **kwargs)\n",
    "        else:\n",
    "            exit_code = restart_with_reloader()\n",
    "            sys.exit(exit_code)\n",
    "    except KeyboardInterrupt:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}