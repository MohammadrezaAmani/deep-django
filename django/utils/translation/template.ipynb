{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from io import StringIO\n",
    "\n",
    "from django.template.base import Lexer, TokenType\n",
    "from django.utils.regex_helper import _lazy_re_compile\n",
    "\n",
    "from . import TranslatorCommentWarning, trim_whitespace\n",
    "\n",
    "TRANSLATOR_COMMENT_MARK = \"Translators\"\n",
    "\n",
    "dot_re = _lazy_re_compile(r\"\\S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blankout(src, char):\n",
    "    \"\"\"\n",
    "    Change every non-whitespace character to the given char.\n",
    "    Used in the templatize function.\n",
    "    \"\"\"\n",
    "    return dot_re.sub(char, src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_re = _lazy_re_compile(r\"\"\"^\\s+.*context\\s+((?:\"[^\"]*?\")|(?:'[^']*?'))\\s*\"\"\")\n",
    "inline_re = _lazy_re_compile(\n",
    "    # Match the trans/translate 'some text' part.\n",
    "    r\"\"\"^\\s*trans(?:late)?\\s+((?:\"[^\"]*?\")|(?:'[^']*?'))\"\"\"\n",
    "    # Match and ignore optional filters\n",
    "    r\"\"\"(?:\\s*\\|\\s*[^\\s:]+(?::(?:[^\\s'\":]+|(?:\"[^\"]*?\")|(?:'[^']*?')))?)*\"\"\"\n",
    "    # Match the optional context part\n",
    "    r\"\"\"(\\s+.*context\\s+((?:\"[^\"]*?\")|(?:'[^']*?')))?\\s*\"\"\"\n",
    ")\n",
    "block_re = _lazy_re_compile(\n",
    "    r\"\"\"^\\s*blocktrans(?:late)?(\\s+.*context\\s+((?:\"[^\"]*?\")|(?:'[^']*?')))?(?:\\s+|$)\"\"\"\n",
    ")\n",
    "endblock_re = _lazy_re_compile(r\"\"\"^\\s*endblocktrans(?:late)?$\"\"\")\n",
    "plural_re = _lazy_re_compile(r\"\"\"^\\s*plural$\"\"\")\n",
    "constant_re = _lazy_re_compile(r\"\"\"_\\(((?:\".*?\")|(?:'.*?'))\\)\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def templatize(src, origin=None):\n",
    "    \"\"\"\n",
    "    Turn a Django template into something that is understood by xgettext. It\n",
    "    does so by translating the Django translation tags into standard gettext\n",
    "    function invocations.\n",
    "    \"\"\"\n",
    "    out = StringIO(\"\")\n",
    "    message_context = None\n",
    "    intrans = False\n",
    "    inplural = False\n",
    "    trimmed = False\n",
    "    singular = []\n",
    "    plural = []\n",
    "    incomment = False\n",
    "    comment = []\n",
    "    lineno_comment_map = {}\n",
    "    comment_lineno_cache = None\n",
    "    # Adding the u prefix allows gettext to recognize the string (#26093).\n",
    "    raw_prefix = \"u\"\n",
    "\n",
    "    def join_tokens(tokens, trim=False):\n",
    "        message = \"\".join(tokens)\n",
    "        if trim:\n",
    "            message = trim_whitespace(message)\n",
    "        return message\n",
    "\n",
    "    for t in Lexer(src).tokenize():\n",
    "        if incomment:\n",
    "            if t.token_type == TokenType.BLOCK and t.contents == \"endcomment\":\n",
    "                content = \"\".join(comment)\n",
    "                translators_comment_start = None\n",
    "                for lineno, line in enumerate(content.splitlines(True)):\n",
    "                    if line.lstrip().startswith(TRANSLATOR_COMMENT_MARK):\n",
    "                        translators_comment_start = lineno\n",
    "                for lineno, line in enumerate(content.splitlines(True)):\n",
    "                    if (\n",
    "                        translators_comment_start is not None\n",
    "                        and lineno >= translators_comment_start\n",
    "                    ):\n",
    "                        out.write(\" # %s\" % line)\n",
    "                    else:\n",
    "                        out.write(\" #\\n\")\n",
    "                incomment = False\n",
    "                comment = []\n",
    "            else:\n",
    "                comment.append(t.contents)\n",
    "        elif intrans:\n",
    "            if t.token_type == TokenType.BLOCK:\n",
    "                endbmatch = endblock_re.match(t.contents)\n",
    "                pluralmatch = plural_re.match(t.contents)\n",
    "                if endbmatch:\n",
    "                    if inplural:\n",
    "                        if message_context:\n",
    "                            out.write(\n",
    "                                \" npgettext({p}{!r}, {p}{!r}, {p}{!r},count) \".format(\n",
    "                                    message_context,\n",
    "                                    join_tokens(singular, trimmed),\n",
    "                                    join_tokens(plural, trimmed),\n",
    "                                    p=raw_prefix,\n",
    "                                )\n",
    "                            )\n",
    "                        else:\n",
    "                            out.write(\n",
    "                                \" ngettext({p}{!r}, {p}{!r}, count) \".format(\n",
    "                                    join_tokens(singular, trimmed),\n",
    "                                    join_tokens(plural, trimmed),\n",
    "                                    p=raw_prefix,\n",
    "                                )\n",
    "                            )\n",
    "                        for part in singular:\n",
    "                            out.write(blankout(part, \"S\"))\n",
    "                        for part in plural:\n",
    "                            out.write(blankout(part, \"P\"))\n",
    "                    else:\n",
    "                        if message_context:\n",
    "                            out.write(\n",
    "                                \" pgettext({p}{!r}, {p}{!r}) \".format(\n",
    "                                    message_context,\n",
    "                                    join_tokens(singular, trimmed),\n",
    "                                    p=raw_prefix,\n",
    "                                )\n",
    "                            )\n",
    "                        else:\n",
    "                            out.write(\n",
    "                                \" gettext({p}{!r}) \".format(\n",
    "                                    join_tokens(singular, trimmed),\n",
    "                                    p=raw_prefix,\n",
    "                                )\n",
    "                            )\n",
    "                        for part in singular:\n",
    "                            out.write(blankout(part, \"S\"))\n",
    "                    message_context = None\n",
    "                    intrans = False\n",
    "                    inplural = False\n",
    "                    singular = []\n",
    "                    plural = []\n",
    "                elif pluralmatch:\n",
    "                    inplural = True\n",
    "                else:\n",
    "                    filemsg = \"\"\n",
    "                    if origin:\n",
    "                        filemsg = \"file %s, \" % origin\n",
    "                    raise SyntaxError(\n",
    "                        \"Translation blocks must not include other block tags: \"\n",
    "                        \"%s (%sline %d)\" % (t.contents, filemsg, t.lineno)\n",
    "                    )\n",
    "            elif t.token_type == TokenType.VAR:\n",
    "                if inplural:\n",
    "                    plural.append(\"%%(%s)s\" % t.contents)\n",
    "                else:\n",
    "                    singular.append(\"%%(%s)s\" % t.contents)\n",
    "            elif t.token_type == TokenType.TEXT:\n",
    "                contents = t.contents.replace(\"%\", \"%%\")\n",
    "                if inplural:\n",
    "                    plural.append(contents)\n",
    "                else:\n",
    "                    singular.append(contents)\n",
    "        else:\n",
    "            # Handle comment tokens (`{# ... #}`) plus other constructs on\n",
    "            # the same line:\n",
    "            if comment_lineno_cache is not None:\n",
    "                cur_lineno = t.lineno + t.contents.count(\"\\n\")\n",
    "                if comment_lineno_cache == cur_lineno:\n",
    "                    if t.token_type != TokenType.COMMENT:\n",
    "                        for c in lineno_comment_map[comment_lineno_cache]:\n",
    "                            filemsg = \"\"\n",
    "                            if origin:\n",
    "                                filemsg = \"file %s, \" % origin\n",
    "                            warn_msg = (\n",
    "                                \"The translator-targeted comment '%s' \"\n",
    "                                \"(%sline %d) was ignored, because it wasn't \"\n",
    "                                \"the last item on the line.\"\n",
    "                            ) % (c, filemsg, comment_lineno_cache)\n",
    "                            warnings.warn(warn_msg, TranslatorCommentWarning)\n",
    "                        lineno_comment_map[comment_lineno_cache] = []\n",
    "                else:\n",
    "                    out.write(\n",
    "                        \"# %s\" % \" | \".join(lineno_comment_map[comment_lineno_cache])\n",
    "                    )\n",
    "                comment_lineno_cache = None\n",
    "\n",
    "            if t.token_type == TokenType.BLOCK:\n",
    "                imatch = inline_re.match(t.contents)\n",
    "                bmatch = block_re.match(t.contents)\n",
    "                cmatches = constant_re.findall(t.contents)\n",
    "                if imatch:\n",
    "                    g = imatch[1]\n",
    "                    if g[0] == '\"':\n",
    "                        g = g.strip('\"')\n",
    "                    elif g[0] == \"'\":\n",
    "                        g = g.strip(\"'\")\n",
    "                    g = g.replace(\"%\", \"%%\")\n",
    "                    if imatch[2]:\n",
    "                        # A context is provided\n",
    "                        context_match = context_re.match(imatch[2])\n",
    "                        message_context = context_match[1]\n",
    "                        if message_context[0] == '\"':\n",
    "                            message_context = message_context.strip('\"')\n",
    "                        elif message_context[0] == \"'\":\n",
    "                            message_context = message_context.strip(\"'\")\n",
    "                        out.write(\n",
    "                            \" pgettext({p}{!r}, {p}{!r}) \".format(\n",
    "                                message_context, g, p=raw_prefix\n",
    "                            )\n",
    "                        )\n",
    "                        message_context = None\n",
    "                    else:\n",
    "                        out.write(\" gettext({p}{!r}) \".format(g, p=raw_prefix))\n",
    "                elif bmatch:\n",
    "                    for fmatch in constant_re.findall(t.contents):\n",
    "                        out.write(\" _(%s) \" % fmatch)\n",
    "                    if bmatch[1]:\n",
    "                        # A context is provided\n",
    "                        context_match = context_re.match(bmatch[1])\n",
    "                        message_context = context_match[1]\n",
    "                        if message_context[0] == '\"':\n",
    "                            message_context = message_context.strip('\"')\n",
    "                        elif message_context[0] == \"'\":\n",
    "                            message_context = message_context.strip(\"'\")\n",
    "                    intrans = True\n",
    "                    inplural = False\n",
    "                    trimmed = \"trimmed\" in t.split_contents()\n",
    "                    singular = []\n",
    "                    plural = []\n",
    "                elif cmatches:\n",
    "                    for cmatch in cmatches:\n",
    "                        out.write(\" _(%s) \" % cmatch)\n",
    "                elif t.contents == \"comment\":\n",
    "                    incomment = True\n",
    "                else:\n",
    "                    out.write(blankout(t.contents, \"B\"))\n",
    "            elif t.token_type == TokenType.VAR:\n",
    "                parts = t.contents.split(\"|\")\n",
    "                cmatch = constant_re.match(parts[0])\n",
    "                if cmatch:\n",
    "                    out.write(\" _(%s) \" % cmatch[1])\n",
    "                for p in parts[1:]:\n",
    "                    if p.find(\":_(\") >= 0:\n",
    "                        out.write(\" %s \" % p.split(\":\", 1)[1])\n",
    "                    else:\n",
    "                        out.write(blankout(p, \"F\"))\n",
    "            elif t.token_type == TokenType.COMMENT:\n",
    "                if t.contents.lstrip().startswith(TRANSLATOR_COMMENT_MARK):\n",
    "                    lineno_comment_map.setdefault(t.lineno, []).append(t.contents)\n",
    "                    comment_lineno_cache = t.lineno\n",
    "            else:\n",
    "                out.write(blankout(t.contents, \"X\"))\n",
    "    return out.getvalue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}