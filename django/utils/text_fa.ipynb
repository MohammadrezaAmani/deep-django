{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import re\n",
    "import secrets\n",
    "import unicodedata\n",
    "from gzip import GzipFile\n",
    "from gzip import compress as gzip_compress\n",
    "from io import BytesIO\n",
    "\n",
    "from django.core.exceptions import SuspiciousFileOperation\n",
    "from django.utils.functional import SimpleLazyObject, keep_lazy_text, lazy\n",
    "from django.utils.regex_helper import _lazy_re_compile\n",
    "from django.utils.translation import gettext as _\n",
    "from django.utils.translation import gettext_lazy, pgettext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keep_lazy_text\n",
    "def capfirst(x):\n",
    "    \"\"\"Capitalize the first letter of a string.\"\"\"\n",
    "    if not x:\n",
    "        return x\n",
    "    if not isinstance(x, str):\n",
    "        x = str(x)\n",
    "    return x[0].upper() + x[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Begin security-related performance workaround -----\n",
    "\n",
    "# We used to have, below\n",
    "#\n",
    "# re_words = _lazy_re_compile(r\"<[^>]+?>|([^<>\\s]+)\", re.S)\n",
    "#\n",
    "# But it was shown that this regex, in the way we use it here, has some\n",
    "# catastrophic edge-case performance features. Namely, when it is applied to\n",
    "# text with only open brackets \"<<<...\". The class below provides the services\n",
    "# and correct answers for the use cases, but in these edge cases does it much\n",
    "# faster.\n",
    "re_notag = _lazy_re_compile(r\"([^<>\\s]+)\", re.S)\n",
    "re_prt = _lazy_re_compile(r\"<|([^<>\\s]+)\", re.S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordsRegex:\n",
    "    @staticmethod\n",
    "    def search(text, pos):\n",
    "        # Look for \"<\" or a non-tag word.\n",
    "        partial = re_prt.search(text, pos)\n",
    "        if partial is None or partial[1] is not None:\n",
    "            return partial\n",
    "\n",
    "        # \"<\" was found, look for a closing \">\".\n",
    "        end = text.find(\">\", partial.end(0))\n",
    "        if end < 0:\n",
    "            # \">\" cannot be found, look for a word.\n",
    "            return re_notag.search(text, pos + 1)\n",
    "        else:\n",
    "            # \"<\" followed by a \">\" was found -- fake a match.\n",
    "            end += 1\n",
    "            return FakeMatch(text[partial.start(0) : end], end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeMatch:\n",
    "    __slots__ = [\"_text\", \"_end\"]\n",
    "\n",
    "    def end(self, group=0):\n",
    "        assert group == 0, \"This specific object takes only group=0\"\n",
    "        return self._end\n",
    "\n",
    "    def __getitem__(self, group):\n",
    "        if group == 1:\n",
    "            return None\n",
    "        assert group == 0, \"This specific object takes only group in {0,1}\"\n",
    "        return self._text\n",
    "\n",
    "    def __init__(self, text, end):\n",
    "        self._text, self._end = text, end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- End security-related performance workaround -----\n",
    "\n",
    "# Set up regular expressions.\n",
    "re_words = WordsRegex\n",
    "re_chars = _lazy_re_compile(r\"<[^>]+?>|(.)\", re.S)\n",
    "re_tag = _lazy_re_compile(r\"<(/)?(\\S+?)(?:(\\s*/)|\\s.*?)?>\", re.S)\n",
    "re_newlines = _lazy_re_compile(r\"\\r\\n|\\r\")  # Used in normalize_newlines\n",
    "re_camel_case = _lazy_re_compile(r\"(((?<=[a-z])[A-Z])|([A-Z](?![A-Z]|$)))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keep_lazy_text\n",
    "def wrap(text, width):\n",
    "    \"\"\"\n",
    "    A word-wrap function that preserves existing line breaks. Expects that\n",
    "    existing line breaks are posix newlines.\n",
    "\n",
    "    Preserve all white space except added line breaks consume the space on\n",
    "    which they break the line.\n",
    "\n",
    "    Don't wrap long words, thus the output text may have lines longer than\n",
    "    ``width``.\n",
    "    \"\"\"\n",
    "\n",
    "    def _generator():\n",
    "        for line in text.splitlines(True):  # True keeps trailing linebreaks\n",
    "            max_width = min((line.endswith(\"\\n\") and width + 1 or width), width)\n",
    "            while len(line) > max_width:\n",
    "                space = line[: max_width + 1].rfind(\" \") + 1\n",
    "                if space == 0:\n",
    "                    space = line.find(\" \") + 1\n",
    "                    if space == 0:\n",
    "                        yield line\n",
    "                        line = \"\"\n",
    "                        break\n",
    "                yield \"%s\\n\" % line[: space - 1]\n",
    "                line = line[space:]\n",
    "                max_width = min((line.endswith(\"\\n\") and width + 1 or width), width)\n",
    "            if line:\n",
    "                yield line\n",
    "\n",
    "    return \"\".join(_generator())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_truncation_text(text, truncate=None):\n",
    "    if truncate is None:\n",
    "        truncate = pgettext(\n",
    "            \"String to return when truncating text\", \"%(truncated_text)sâ€¦\"\n",
    "        )\n",
    "    if \"%(truncated_text)s\" in truncate:\n",
    "        return truncate % {\"truncated_text\": text}\n",
    "    # The truncation text didn't contain the %(truncated_text)s string\n",
    "    # replacement argument so just append it to the text.\n",
    "    if text.endswith(truncate):\n",
    "        # But don't append the truncation text if the current text already ends\n",
    "        # in this.\n",
    "        return text\n",
    "    return f\"{text}{truncate}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Truncator(SimpleLazyObject):\n",
    "    \"\"\"\n",
    "    An object used to truncate text, either by characters or words.\n",
    "\n",
    "    When truncating HTML text (either chars or words), input will be limited to\n",
    "    at most `MAX_LENGTH_HTML` characters.\n",
    "    \"\"\"\n",
    "\n",
    "    # 5 million characters are approximately 4000 text pages or 3 web pages.\n",
    "    MAX_LENGTH_HTML = 5_000_000\n",
    "\n",
    "    def __init__(self, text):\n",
    "        super().__init__(lambda: str(text))\n",
    "\n",
    "    def chars(self, num, truncate=None, html=False):\n",
    "        \"\"\"\n",
    "        Return the text truncated to be no longer than the specified number\n",
    "        of characters.\n",
    "\n",
    "        `truncate` specifies what should be used to notify that the string has\n",
    "        been truncated, defaulting to a translatable string of an ellipsis.\n",
    "        \"\"\"\n",
    "        self._setup()\n",
    "        length = int(num)\n",
    "        text = unicodedata.normalize(\"NFC\", self._wrapped)\n",
    "\n",
    "        # Calculate the length to truncate to (max length - end_text length)\n",
    "        truncate_len = length\n",
    "        for char in add_truncation_text(\"\", truncate):\n",
    "            if not unicodedata.combining(char):\n",
    "                truncate_len -= 1\n",
    "                if truncate_len == 0:\n",
    "                    break\n",
    "        if html:\n",
    "            return self._truncate_html(length, truncate, text, truncate_len, False)\n",
    "        return self._text_chars(length, truncate, text, truncate_len)\n",
    "\n",
    "    def _text_chars(self, length, truncate, text, truncate_len):\n",
    "        \"\"\"Truncate a string after a certain number of chars.\"\"\"\n",
    "        s_len = 0\n",
    "        end_index = None\n",
    "        for i, char in enumerate(text):\n",
    "            if unicodedata.combining(char):\n",
    "                # Don't consider combining characters\n",
    "                # as adding to the string length\n",
    "                continue\n",
    "            s_len += 1\n",
    "            if end_index is None and s_len > truncate_len:\n",
    "                end_index = i\n",
    "            if s_len > length:\n",
    "                # Return the truncated string\n",
    "                return add_truncation_text(text[: end_index or 0], truncate)\n",
    "\n",
    "        # Return the original string since no truncation was necessary\n",
    "        return text\n",
    "\n",
    "    def words(self, num, truncate=None, html=False):\n",
    "        \"\"\"\n",
    "        Truncate a string after a certain number of words. `truncate` specifies\n",
    "        what should be used to notify that the string has been truncated,\n",
    "        defaulting to ellipsis.\n",
    "        \"\"\"\n",
    "        self._setup()\n",
    "        length = int(num)\n",
    "        if html:\n",
    "            return self._truncate_html(length, truncate, self._wrapped, length, True)\n",
    "        return self._text_words(length, truncate)\n",
    "\n",
    "    def _text_words(self, length, truncate):\n",
    "        \"\"\"\n",
    "        Truncate a string after a certain number of words.\n",
    "\n",
    "        Strip newlines in the string.\n",
    "        \"\"\"\n",
    "        words = self._wrapped.split()\n",
    "        if len(words) > length:\n",
    "            words = words[:length]\n",
    "            return add_truncation_text(\" \".join(words), truncate)\n",
    "        return \" \".join(words)\n",
    "\n",
    "    def _truncate_html(self, length, truncate, text, truncate_len, words):\n",
    "        \"\"\"\n",
    "        Truncate HTML to a certain number of chars (not counting tags and\n",
    "        comments), or, if words is True, then to a certain number of words.\n",
    "        Close opened tags if they were correctly closed in the given HTML.\n",
    "\n",
    "        Preserve newlines in the HTML.\n",
    "        \"\"\"\n",
    "        if words and length <= 0:\n",
    "            return \"\"\n",
    "\n",
    "        size_limited = False\n",
    "        if len(text) > self.MAX_LENGTH_HTML:\n",
    "            text = text[: self.MAX_LENGTH_HTML]\n",
    "            size_limited = True\n",
    "\n",
    "        html4_singlets = (\n",
    "            \"br\",\n",
    "            \"col\",\n",
    "            \"link\",\n",
    "            \"base\",\n",
    "            \"img\",\n",
    "            \"param\",\n",
    "            \"area\",\n",
    "            \"hr\",\n",
    "            \"input\",\n",
    "        )\n",
    "\n",
    "        # Count non-HTML chars/words and keep note of open tags\n",
    "        pos = 0\n",
    "        end_text_pos = 0\n",
    "        current_len = 0\n",
    "        open_tags = []\n",
    "\n",
    "        regex = re_words if words else re_chars\n",
    "\n",
    "        while current_len <= length:\n",
    "            m = regex.search(text, pos)\n",
    "            if not m:\n",
    "                # Checked through whole string\n",
    "                break\n",
    "            pos = m.end(0)\n",
    "            if m[1]:\n",
    "                # It's an actual non-HTML word or char\n",
    "                current_len += 1\n",
    "                if current_len == truncate_len:\n",
    "                    end_text_pos = pos\n",
    "                continue\n",
    "            # Check for tag\n",
    "            tag = re_tag.match(m[0])\n",
    "            if not tag or current_len >= truncate_len:\n",
    "                # Don't worry about non tags or tags after our truncate point\n",
    "                continue\n",
    "            closing_tag, tagname, self_closing = tag.groups()\n",
    "            # Element names are always case-insensitive\n",
    "            tagname = tagname.lower()\n",
    "            if self_closing or tagname in html4_singlets:\n",
    "                pass\n",
    "            elif closing_tag:\n",
    "                # Check for match in open tags list\n",
    "                try:\n",
    "                    i = open_tags.index(tagname)\n",
    "                except ValueError:\n",
    "                    pass\n",
    "                else:\n",
    "                    # SGML: An end tag closes, back to the matching start tag,\n",
    "                    # all unclosed intervening start tags with omitted end tags\n",
    "                    open_tags = open_tags[i + 1 :]\n",
    "            else:\n",
    "                # Add it to the start of the open tags list\n",
    "                open_tags.insert(0, tagname)\n",
    "\n",
    "        truncate_text = add_truncation_text(\"\", truncate)\n",
    "\n",
    "        if current_len <= length:\n",
    "            if size_limited and truncate_text:\n",
    "                text += truncate_text\n",
    "            return text\n",
    "\n",
    "        out = text[:end_text_pos]\n",
    "        if truncate_text:\n",
    "            out += truncate_text\n",
    "        # Close any tags still open\n",
    "        for tag in open_tags:\n",
    "            out += \"</%s>\" % tag\n",
    "        # Return string\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keep_lazy_text\n",
    "def get_valid_filename(name):\n",
    "    \"\"\"\n",
    "    Return the given string converted to a string that can be used for a clean\n",
    "    filename. Remove leading and trailing spaces; convert other spaces to\n",
    "    underscores; and remove anything that is not an alphanumeric, dash,\n",
    "    underscore, or dot.\n",
    "    >>> get_valid_filename(\"john's portrait in 2004.jpg\")\n",
    "    'johns_portrait_in_2004.jpg'\n",
    "    \"\"\"\n",
    "    s = str(name).strip().replace(\" \", \"_\")\n",
    "    s = re.sub(r\"(?u)[^-\\w.]\", \"\", s)\n",
    "    if s in {\"\", \".\", \"..\"}:\n",
    "        raise SuspiciousFileOperation(\"Could not derive file name from '%s'\" % name)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keep_lazy_text\n",
    "def get_text_list(list_, last_word=gettext_lazy(\"or\")):\n",
    "    \"\"\"\n",
    "    >>> get_text_list(['a', 'b', 'c', 'd'])\n",
    "    'a, b, c or d'\n",
    "    >>> get_text_list(['a', 'b', 'c'], 'and')\n",
    "    'a, b and c'\n",
    "    >>> get_text_list(['a', 'b'], 'and')\n",
    "    'a and b'\n",
    "    >>> get_text_list(['a'])\n",
    "    'a'\n",
    "    >>> get_text_list([])\n",
    "    ''\n",
    "    \"\"\"\n",
    "    if not list_:\n",
    "        return \"\"\n",
    "    if len(list_) == 1:\n",
    "        return str(list_[0])\n",
    "    return \"%s %s %s\" % (\n",
    "        # Translators: This string is used as a separator between list elements\n",
    "        _(\", \").join(str(i) for i in list_[:-1]),\n",
    "        str(last_word),\n",
    "        str(list_[-1]),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keep_lazy_text\n",
    "def normalize_newlines(text):\n",
    "    \"\"\"Normalize CRLF and CR newlines to just LF.\"\"\"\n",
    "    return re_newlines.sub(\"\\n\", str(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keep_lazy_text\n",
    "def phone2numeric(phone):\n",
    "    \"\"\"Convert a phone number with letters into its numeric equivalent.\"\"\"\n",
    "    char2number = {\n",
    "        \"a\": \"2\",\n",
    "        \"b\": \"2\",\n",
    "        \"c\": \"2\",\n",
    "        \"d\": \"3\",\n",
    "        \"e\": \"3\",\n",
    "        \"f\": \"3\",\n",
    "        \"g\": \"4\",\n",
    "        \"h\": \"4\",\n",
    "        \"i\": \"4\",\n",
    "        \"j\": \"5\",\n",
    "        \"k\": \"5\",\n",
    "        \"l\": \"5\",\n",
    "        \"m\": \"6\",\n",
    "        \"n\": \"6\",\n",
    "        \"o\": \"6\",\n",
    "        \"p\": \"7\",\n",
    "        \"q\": \"7\",\n",
    "        \"r\": \"7\",\n",
    "        \"s\": \"7\",\n",
    "        \"t\": \"8\",\n",
    "        \"u\": \"8\",\n",
    "        \"v\": \"8\",\n",
    "        \"w\": \"9\",\n",
    "        \"x\": \"9\",\n",
    "        \"y\": \"9\",\n",
    "        \"z\": \"9\",\n",
    "    }\n",
    "    return \"\".join(char2number.get(c, c) for c in phone.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_random_filename(max_random_bytes):\n",
    "    return b\"a\" * secrets.randbelow(max_random_bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_string(s, *, max_random_bytes=None):\n",
    "    compressed_data = gzip_compress(s, compresslevel=6, mtime=0)\n",
    "\n",
    "    if not max_random_bytes:\n",
    "        return compressed_data\n",
    "\n",
    "    compressed_view = memoryview(compressed_data)\n",
    "    header = bytearray(compressed_view[:10])\n",
    "    header[3] = gzip.FNAME\n",
    "\n",
    "    filename = _get_random_filename(max_random_bytes) + b\"\\x00\"\n",
    "\n",
    "    return bytes(header) + filename + compressed_view[10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreamingBuffer(BytesIO):\n",
    "    def read(self):\n",
    "        ret = self.getvalue()\n",
    "        self.seek(0)\n",
    "        self.truncate()\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like compress_string, but for iterators of strings.\n",
    "def compress_sequence(sequence, *, max_random_bytes=None):\n",
    "    buf = StreamingBuffer()\n",
    "    filename = _get_random_filename(max_random_bytes) if max_random_bytes else None\n",
    "    with GzipFile(\n",
    "        filename=filename, mode=\"wb\", compresslevel=6, fileobj=buf, mtime=0\n",
    "    ) as zfile:\n",
    "        # Output headers...\n",
    "        yield buf.read()\n",
    "        for item in sequence:\n",
    "            zfile.write(item)\n",
    "            data = buf.read()\n",
    "            if data:\n",
    "                yield data\n",
    "    yield buf.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expression to match some_token and some_token=\"with spaces\" (and similarly\n",
    "# for single-quoted strings).\n",
    "smart_split_re = _lazy_re_compile(\n",
    "    r\"\"\"\n",
    "    ((?:\n",
    "        [^\\s'\"]*\n",
    "        (?:\n",
    "            (?:\"(?:[^\"\\\\]|\\\\.)*\" | '(?:[^'\\\\]|\\\\.)*')\n",
    "            [^\\s'\"]*\n",
    "        )+\n",
    "    ) | \\S+)\n",
    "\"\"\",\n",
    "    re.VERBOSE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_split(text):\n",
    "    r\"\"\"\n",
    "    Generator that splits a string by spaces, leaving quoted phrases together.\n",
    "    Supports both single and double quotes, and supports escaping quotes with\n",
    "    backslashes. In the output, strings will keep their initial and trailing\n",
    "    quote marks and escaped quotes will remain escaped (the results can then\n",
    "    be further processed with unescape_string_literal()).\n",
    "\n",
    "    >>> list(smart_split(r'This is \"a person\\'s\" test.'))\n",
    "    ['This', 'is', '\"a person\\\\\\'s\"', 'test.']\n",
    "    >>> list(smart_split(r\"Another 'person\\'s' test.\"))\n",
    "    ['Another', \"'person\\\\'s'\", 'test.']\n",
    "    >>> list(smart_split(r'A \"\\\"funky\\\" style\" test.'))\n",
    "    ['A', '\"\\\\\"funky\\\\\" style\"', 'test.']\n",
    "    \"\"\"\n",
    "    for bit in smart_split_re.finditer(str(text)):\n",
    "        yield bit[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keep_lazy_text\n",
    "def unescape_string_literal(s):\n",
    "    r\"\"\"\n",
    "    Convert quoted string literals to unquoted strings with escaped quotes and\n",
    "    backslashes unquoted::\n",
    "\n",
    "        >>> unescape_string_literal('\"abc\"')\n",
    "        'abc'\n",
    "        >>> unescape_string_literal(\"'abc'\")\n",
    "        'abc'\n",
    "        >>> unescape_string_literal('\"a \\\"bc\\\"\"')\n",
    "        'a \"bc\"'\n",
    "        >>> unescape_string_literal(\"'\\'ab\\' c'\")\n",
    "        \"'ab' c\"\n",
    "    \"\"\"\n",
    "    if not s or s[0] not in \"\\\"'\" or s[-1] != s[0]:\n",
    "        raise ValueError(\"Not a string literal: %r\" % s)\n",
    "    quote = s[0]\n",
    "    return s[1:-1].replace(r\"\\%s\" % quote, quote).replace(r\"\\\\\", \"\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@keep_lazy_text\n",
    "def slugify(value, allow_unicode=False):\n",
    "    \"\"\"\n",
    "    Convert to ASCII if 'allow_unicode' is False. Convert spaces or repeated\n",
    "    dashes to single dashes. Remove characters that aren't alphanumerics,\n",
    "    underscores, or hyphens. Convert to lowercase. Also strip leading and\n",
    "    trailing whitespace, dashes, and underscores.\n",
    "    \"\"\"\n",
    "    value = str(value)\n",
    "    if allow_unicode:\n",
    "        value = unicodedata.normalize(\"NFKC\", value)\n",
    "    else:\n",
    "        value = (\n",
    "            unicodedata.normalize(\"NFKD\", value)\n",
    "            .encode(\"ascii\", \"ignore\")\n",
    "            .decode(\"ascii\")\n",
    "        )\n",
    "    value = re.sub(r\"[^\\w\\s-]\", \"\", value.lower())\n",
    "    return re.sub(r\"[-\\s]+\", \"-\", value).strip(\"-_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camel_case_to_spaces(value):\n",
    "    \"\"\"\n",
    "    Split CamelCase and convert to lowercase. Strip surrounding whitespace.\n",
    "    \"\"\"\n",
    "    return re_camel_case.sub(r\" \\1\", value).strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _format_lazy(format_string, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Apply str.format() on 'format_string' where format_string, args,\n",
    "    and/or kwargs might be lazy.\n",
    "    \"\"\"\n",
    "    return format_string.format(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format_lazy = lazy(_format_lazy, str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}