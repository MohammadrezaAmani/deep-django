{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Copyright 2020 Confluent Inc.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#\n",
    "\n",
    "from confluent_kafka.cimpl import Producer as _ProducerImpl\n",
    "\n",
    "from .error import KeySerializationError, ValueSerializationError\n",
    "from .serialization import MessageField, SerializationContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SerializingProducer(_ProducerImpl):\n",
    "    \"\"\"\n",
    "    A high level Kafka producer with serialization capabilities.\n",
    "\n",
    "    `This class is experimental and likely to be removed, or subject to incompatible API\n",
    "    changes in future versions of the library. To avoid breaking changes on upgrading, we\n",
    "    recommend using serializers directly.`\n",
    "\n",
    "    Derived from the :py:class:`Producer` class, overriding the :py:func:`Producer.produce`\n",
    "    method to add serialization capabilities.\n",
    "\n",
    "    Additional configuration properties:\n",
    "\n",
    "    +-------------------------+---------------------+-----------------------------------------------------+\n",
    "    | Property Name           | Type                | Description                                         |\n",
    "    +=========================+=====================+=====================================================+\n",
    "    |                         |                     | Callable(obj, SerializationContext) -> bytes        |\n",
    "    | ``key.serializer``      | callable            |                                                     |\n",
    "    |                         |                     | Serializer used for message keys.                   |\n",
    "    +-------------------------+---------------------+-----------------------------------------------------+\n",
    "    |                         |                     | Callable(obj, SerializationContext) -> bytes        |\n",
    "    | ``value.serializer``    | callable            |                                                     |\n",
    "    |                         |                     | Serializer used for message values.                 |\n",
    "    +-------------------------+---------------------+-----------------------------------------------------+\n",
    "\n",
    "    Serializers for string, integer and double (:py:class:`StringSerializer`, :py:class:`IntegerSerializer`\n",
    "    and :py:class:`DoubleSerializer`) are supplied out-of-the-box in the ``confluent_kafka.serialization``\n",
    "    namespace.\n",
    "\n",
    "    Serializers for Protobuf, JSON Schema and Avro (:py:class:`ProtobufSerializer`, :py:class:`JSONSerializer`\n",
    "    and :py:class:`AvroSerializer`) with Confluent Schema Registry integration are supplied out-of-the-box\n",
    "    in the ``confluent_kafka.schema_registry`` namespace.\n",
    "\n",
    "    See Also:\n",
    "        - The :ref:`Configuration Guide <pythonclient_configuration>` for in depth information on how to configure the client.\n",
    "        - `CONFIGURATION.md <https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md>`_ for a comprehensive set of configuration properties.\n",
    "        - `STATISTICS.md <https://github.com/edenhill/librdkafka/blob/master/STATISTICS.md>`_ for detailed information on the statistics provided by stats_cb\n",
    "        - The :py:class:`Producer` class for inherited methods.\n",
    "\n",
    "    Args:\n",
    "        conf (producer): SerializingProducer configuration.\n",
    "    \"\"\"  # noqa E501\n",
    "\n",
    "    def __init__(self, conf):\n",
    "        conf_copy = conf.copy()\n",
    "\n",
    "        self._key_serializer = conf_copy.pop(\"key.serializer\", None)\n",
    "        self._value_serializer = conf_copy.pop(\"value.serializer\", None)\n",
    "\n",
    "        super(SerializingProducer, self).__init__(conf_copy)\n",
    "\n",
    "    def produce(\n",
    "        self,\n",
    "        topic,\n",
    "        key=None,\n",
    "        value=None,\n",
    "        partition=-1,\n",
    "        on_delivery=None,\n",
    "        timestamp=0,\n",
    "        headers=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Produce a message.\n",
    "\n",
    "        This is an asynchronous operation. An application may use the\n",
    "        ``on_delivery`` argument to pass a function (or lambda) that will be\n",
    "        called from :py:func:`SerializingProducer.poll` when the message has\n",
    "        been successfully delivered or permanently fails delivery.\n",
    "\n",
    "        Note:\n",
    "            Currently message headers are not supported on the message returned to\n",
    "            the callback. The ``msg.headers()`` will return None even if the\n",
    "            original message had headers set.\n",
    "\n",
    "        Args:\n",
    "            topic (str): Topic to produce message to.\n",
    "\n",
    "            key (object, optional): Message payload key.\n",
    "\n",
    "            value (object, optional): Message payload value.\n",
    "\n",
    "            partition (int, optional): Partition to produce to, else the\n",
    "                configured built-in partitioner will be used.\n",
    "\n",
    "            on_delivery (callable(KafkaError, Message), optional): Delivery\n",
    "                report callback. Called as a side effect of\n",
    "                :py:func:`SerializingProducer.poll` or\n",
    "                :py:func:`SerializingProducer.flush` on successful or\n",
    "                failed delivery.\n",
    "\n",
    "            timestamp (float, optional): Message timestamp (CreateTime) in\n",
    "                milliseconds since Unix epoch UTC (requires broker >= 0.10.0.0).\n",
    "                Default value is current time.\n",
    "\n",
    "            headers (dict, optional): Message headers. The header key must be\n",
    "                a str while the value must be binary, unicode or None. (Requires\n",
    "                broker version >= 0.11.0.0)\n",
    "\n",
    "        Raises:\n",
    "            BufferError: if the internal producer message queue is full.\n",
    "                (``queue.buffering.max.messages`` exceeded). If this happens\n",
    "                the application should call :py:func:`SerializingProducer.Poll`\n",
    "                and try again.\n",
    "\n",
    "            KeySerializationError: If an error occurs during key serialization.\n",
    "\n",
    "            ValueSerializationError: If an error occurs during value serialization.\n",
    "\n",
    "            KafkaException: For all other errors\n",
    "        \"\"\"\n",
    "\n",
    "        ctx = SerializationContext(topic, MessageField.KEY, headers)\n",
    "        if self._key_serializer is not None:\n",
    "            try:\n",
    "                key = self._key_serializer(key, ctx)\n",
    "            except Exception as se:\n",
    "                raise KeySerializationError(se)\n",
    "        ctx.field = MessageField.VALUE\n",
    "        if self._value_serializer is not None:\n",
    "            try:\n",
    "                value = self._value_serializer(value, ctx)\n",
    "            except Exception as se:\n",
    "                raise ValueSerializationError(se)\n",
    "\n",
    "        super(SerializingProducer, self).produce(\n",
    "            topic,\n",
    "            value,\n",
    "            key,\n",
    "            headers=headers,\n",
    "            partition=partition,\n",
    "            timestamp=timestamp,\n",
    "            on_delivery=on_delivery,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}