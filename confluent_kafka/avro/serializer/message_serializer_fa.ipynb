{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "#\n",
    "# Copyright 2016 Confluent Inc.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# derived from https://github.com/verisign/python-confluent-schemaregistry.git\n",
    "#\n",
    "import io\n",
    "import json\n",
    "import logging\n",
    "import struct\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "import avro\n",
    "import avro.io\n",
    "from confluent_kafka.avro import ClientError\n",
    "from confluent_kafka.avro.serializer import (\n",
    "    KeySerializerError,\n",
    "    SerializerError,\n",
    "    ValueSerializerError,\n",
    ")\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "\n",
    "MAGIC_BYTE = 0\n",
    "\n",
    "HAS_FAST = False\n",
    "try:\n",
    "    from fastavro import schemaless_reader, schemaless_writer\n",
    "    from fastavro.schema import parse_schema\n",
    "\n",
    "    HAS_FAST = True\n",
    "except ImportError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextStringIO(io.BytesIO):\n",
    "    \"\"\"\n",
    "    Wrapper to allow use of StringIO via 'with' constructs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args):\n",
    "        self.close()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MessageSerializer(object):\n",
    "    \"\"\"\n",
    "    A helper class that can serialize and deserialize messages\n",
    "    that need to be encoded or decoded using the schema registry.\n",
    "\n",
    "    All encode_* methods return a buffer that can be sent to kafka.\n",
    "    All decode_* methods expect a buffer received from kafka.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, registry_client, reader_key_schema=None, reader_value_schema=None\n",
    "    ):\n",
    "        self.registry_client = registry_client\n",
    "        self.id_to_decoder_func = {}\n",
    "        self.id_to_writers = {}\n",
    "        self.reader_key_schema = reader_key_schema\n",
    "        self.reader_value_schema = reader_value_schema\n",
    "\n",
    "    # Encoder support\n",
    "    def _get_encoder_func(self, writer_schema):\n",
    "        if HAS_FAST:\n",
    "            schema = json.loads(str(writer_schema))\n",
    "            parsed_schema = parse_schema(schema)\n",
    "            return lambda record, fp: schemaless_writer(fp, parsed_schema, record)\n",
    "        writer = avro.io.DatumWriter(writer_schema)\n",
    "        return lambda record, fp: writer.write(record, avro.io.BinaryEncoder(fp))\n",
    "\n",
    "    def encode_record_with_schema(self, topic, schema, record, is_key=False):\n",
    "        \"\"\"\n",
    "        Given a parsed avro schema, encode a record for the given topic.  The\n",
    "        record is expected to be a dictionary.\n",
    "\n",
    "        The schema is registered with the subject of 'topic-value'\n",
    "        :param str topic: Topic name\n",
    "        :param schema schema: Avro Schema\n",
    "        :param dict record: An object to serialize\n",
    "        :param bool is_key: If the record is a key\n",
    "        :returns: Encoded record with schema ID as bytes\n",
    "        :rtype: bytes\n",
    "        \"\"\"\n",
    "        serialize_err = KeySerializerError if is_key else ValueSerializerError\n",
    "\n",
    "        subject_suffix = \"-key\" if is_key else \"-value\"\n",
    "        # get the latest schema for the subject\n",
    "        subject = topic + subject_suffix\n",
    "        if self.registry_client.auto_register_schemas:\n",
    "            # register it\n",
    "            schema_id = self.registry_client.register(subject, schema)\n",
    "        else:\n",
    "            schema_id = self.registry_client.check_registration(subject, schema)\n",
    "        if not schema_id:\n",
    "            message = \"Unable to retrieve schema id for subject %s\" % (subject)\n",
    "            raise serialize_err(message)\n",
    "\n",
    "        # cache writer\n",
    "        if schema_id not in self.id_to_writers:\n",
    "            self.id_to_writers[schema_id] = self._get_encoder_func(schema)\n",
    "\n",
    "        return self.encode_record_with_schema_id(schema_id, record, is_key=is_key)\n",
    "\n",
    "    def encode_record_with_schema_id(self, schema_id, record, is_key=False):\n",
    "        \"\"\"\n",
    "        Encode a record with a given schema id.  The record must\n",
    "        be a python dictionary.\n",
    "        :param int schema_id: integer ID\n",
    "        :param dict record: An object to serialize\n",
    "        :param bool is_key: If the record is a key\n",
    "        :returns: decoder function\n",
    "        :rtype: func\n",
    "        \"\"\"\n",
    "        serialize_err = KeySerializerError if is_key else ValueSerializerError\n",
    "\n",
    "        # use slow avro\n",
    "        if schema_id not in self.id_to_writers:\n",
    "            # get the writer + schema\n",
    "\n",
    "            try:\n",
    "                schema = self.registry_client.get_by_id(schema_id)\n",
    "                if not schema:\n",
    "                    raise serialize_err(\"Schema does not exist\")\n",
    "                self.id_to_writers[schema_id] = self._get_encoder_func(schema)\n",
    "            except ClientError:\n",
    "                exc_type, exc_value, exc_traceback = sys.exc_info()\n",
    "                raise serialize_err(\n",
    "                    repr(traceback.format_exception(exc_type, exc_value, exc_traceback))\n",
    "                )\n",
    "\n",
    "        # get the writer\n",
    "        writer = self.id_to_writers[schema_id]\n",
    "        with ContextStringIO() as outf:\n",
    "            # Write the magic byte and schema ID in network byte order (big endian)\n",
    "            outf.write(struct.pack(\">bI\", MAGIC_BYTE, schema_id))\n",
    "\n",
    "            # write the record to the rest of the buffer\n",
    "            writer(record, outf)\n",
    "\n",
    "            return outf.getvalue()\n",
    "\n",
    "    # Decoder support\n",
    "    def _get_decoder_func(self, schema_id, payload, is_key=False):\n",
    "        if schema_id in self.id_to_decoder_func:\n",
    "            return self.id_to_decoder_func[schema_id]\n",
    "\n",
    "        # fetch writer schema from schema reg\n",
    "        try:\n",
    "            writer_schema_obj = self.registry_client.get_by_id(schema_id)\n",
    "        except ClientError as e:\n",
    "            raise SerializerError(\n",
    "                \"unable to fetch schema with id %d: %s\" % (schema_id, str(e))\n",
    "            )\n",
    "\n",
    "        if writer_schema_obj is None:\n",
    "            raise SerializerError(\"unable to fetch schema with id %d\" % (schema_id))\n",
    "\n",
    "        curr_pos = payload.tell()\n",
    "\n",
    "        reader_schema_obj = (\n",
    "            self.reader_key_schema if is_key else self.reader_value_schema\n",
    "        )\n",
    "\n",
    "        if HAS_FAST:\n",
    "            # try to use fast avro\n",
    "            try:\n",
    "                fast_avro_writer_schema = parse_schema(\n",
    "                    json.loads(str(writer_schema_obj))\n",
    "                )\n",
    "                if reader_schema_obj is not None:\n",
    "                    fast_avro_reader_schema = parse_schema(\n",
    "                        json.loads(str(reader_schema_obj))\n",
    "                    )\n",
    "                else:\n",
    "                    fast_avro_reader_schema = None\n",
    "                schemaless_reader(payload, fast_avro_writer_schema)\n",
    "\n",
    "                # If we reach this point, this means we have fastavro and it can\n",
    "                # do this deserialization. Rewind since this method just determines\n",
    "                # the reader function and we need to deserialize again along the\n",
    "                # normal path.\n",
    "                payload.seek(curr_pos)\n",
    "\n",
    "                self.id_to_decoder_func[schema_id] = lambda p: schemaless_reader(\n",
    "                    p, fast_avro_writer_schema, fast_avro_reader_schema\n",
    "                )\n",
    "                return self.id_to_decoder_func[schema_id]\n",
    "            except Exception:\n",
    "                log.warning(\n",
    "                    \"Fast avro failed for schema with id %d, falling thru to standard avro\"\n",
    "                    % (schema_id)\n",
    "                )\n",
    "\n",
    "        # here means we should just delegate to slow avro\n",
    "        # rewind\n",
    "        payload.seek(curr_pos)\n",
    "        # Avro DatumReader py2/py3 inconsistency, hence no param keywords\n",
    "        # should be revisited later\n",
    "        # https://github.com/apache/avro/blob/master/lang/py3/avro/io.py#L459\n",
    "        # https://github.com/apache/avro/blob/master/lang/py/src/avro/io.py#L423\n",
    "        # def __init__(self, writers_schema=None, readers_schema=None)\n",
    "        # def __init__(self, writer_schema=None, reader_schema=None)\n",
    "        avro_reader = avro.io.DatumReader(writer_schema_obj, reader_schema_obj)\n",
    "\n",
    "        def decoder(p):\n",
    "            bin_decoder = avro.io.BinaryDecoder(p)\n",
    "            return avro_reader.read(bin_decoder)\n",
    "\n",
    "        self.id_to_decoder_func[schema_id] = decoder\n",
    "        return self.id_to_decoder_func[schema_id]\n",
    "\n",
    "    def decode_message(self, message, is_key=False):\n",
    "        \"\"\"\n",
    "        Decode a message from kafka that has been encoded for use with\n",
    "        the schema registry.\n",
    "        :param str|bytes or None message: message key or value to be decoded\n",
    "        :returns: Decoded message contents.\n",
    "        :rtype dict:\n",
    "        \"\"\"\n",
    "\n",
    "        if message is None:\n",
    "            return None\n",
    "\n",
    "        if len(message) <= 5:\n",
    "            raise SerializerError(\"message is too small to decode\")\n",
    "\n",
    "        with ContextStringIO(message) as payload:\n",
    "            magic, schema_id = struct.unpack(\">bI\", payload.read(5))\n",
    "            if magic != MAGIC_BYTE:\n",
    "                raise SerializerError(\"message does not start with magic byte\")\n",
    "            decoder_func = self._get_decoder_func(schema_id, payload, is_key)\n",
    "            return decoder_func(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}